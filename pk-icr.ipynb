{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4267,"status":"ok","timestamp":1667283630954,"user":{"displayName":"김민상","userId":"05312793560890503441"},"user_tz":-540},"id":"2pPeqLCrK2-U","outputId":"0a40ff52-8e4a-4149-cbfd-7a1129c84214"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.12.1+cu113)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.1.1)\n"]}],"source":["!pip install torch"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10290,"status":"ok","timestamp":1667283641234,"user":{"displayName":"김민상","userId":"05312793560890503441"},"user_tz":-540},"id":"sdw2xGy6K5LF","outputId":"9a9e2328-f2bf-4808-da4d-81737c196f6a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.23.1-py3-none-any.whl (5.3 MB)\n","\u001b[K     |████████████████████████████████| 5.3 MB 4.8 MB/s \n","\u001b[?25hCollecting huggingface-hub<1.0,>=0.10.0\n","  Downloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n","\u001b[K     |████████████████████████████████| 163 kB 55.5 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n","\u001b[K     |████████████████████████████████| 7.6 MB 40.6 MB/s \n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.10.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.10.1 tokenizers-0.13.1 transformers-4.23.1\n"]}],"source":["!pip install transformers"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9550,"status":"ok","timestamp":1667283650779,"user":{"displayName":"김민상","userId":"05312793560890503441"},"user_tz":-540},"id":"AthBZRcwK7ey","outputId":"f9302b43-6c9a-457f-ec9f-2ac8cf1cc100"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting datasets\n","  Downloading datasets-2.6.1-py3-none-any.whl (441 kB)\n","\u001b[K     |████████████████████████████████| 441 kB 5.2 MB/s \n","\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n","Collecting responses<0.19\n","  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n","Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.10.1)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.64.1)\n","Collecting dill<0.3.6\n","  Downloading dill-0.3.5.1-py2.py3-none-any.whl (95 kB)\n","\u001b[K     |████████████████████████████████| 95 kB 5.0 MB/s \n","\u001b[?25hCollecting xxhash\n","  Downloading xxhash-3.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n","\u001b[K     |████████████████████████████████| 212 kB 50.2 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.13.0)\n","Collecting multiprocess\n","  Downloading multiprocess-0.70.14-py37-none-any.whl (115 kB)\n","\u001b[K     |████████████████████████████████| 115 kB 59.1 MB/s \n","\u001b[?25hRequirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.6)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.3)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n","Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (2022.10.0)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.8.1)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.1.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.2.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (22.1.0)\n","Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (0.13.0)\n","Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.1.1)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (6.0.2)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.0.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.8.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.9)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2022.9.24)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n","Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n","  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n","\u001b[K     |████████████████████████████████| 127 kB 69.3 MB/s \n","\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.10.0)\n","Collecting multiprocess\n","  Downloading multiprocess-0.70.13-py37-none-any.whl (115 kB)\n","\u001b[K     |████████████████████████████████| 115 kB 44.8 MB/s \n","\u001b[?25hRequirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2022.5)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n","Installing collected packages: urllib3, dill, xxhash, responses, multiprocess, datasets\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","  Attempting uninstall: dill\n","    Found existing installation: dill 0.3.6\n","    Uninstalling dill-0.3.6:\n","      Successfully uninstalled dill-0.3.6\n","Successfully installed datasets-2.6.1 dill-0.3.5.1 multiprocess-0.70.13 responses-0.18.0 urllib3-1.25.11 xxhash-3.1.0\n"]}],"source":["!pip install datasets"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":2544,"status":"ok","timestamp":1667283653304,"user":{"displayName":"김민상","userId":"05312793560890503441"},"user_tz":-540},"id":"ah7hmBcdK70E"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":5,"status":"aborted","timestamp":1667286739501,"user":{"displayName":"김민상","userId":"05312793560890503441"},"user_tz":-540},"id":"Tectk72TAEJT"},"outputs":[],"source":["import json\n","import os"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1667286742504,"user":{"displayName":"김민상","userId":"05312793560890503441"},"user_tz":-540},"id":"BICJoK-HCKWp","outputId":"cc1e680b-848e-4a16-ccda-90c33f51195b"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content'"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["os.getcwd()"]},{"cell_type":"code","execution_count":32,"metadata":{"executionInfo":{"elapsed":391,"status":"ok","timestamp":1667288130782,"user":{"displayName":"김민상","userId":"05312793560890503441"},"user_tz":-540},"id":"mJur9dmzAfy8"},"outputs":[],"source":["valid_focus = json.load(open('valid_focus.json'))"]},{"cell_type":"code","execution_count":34,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":528,"status":"ok","timestamp":1667289135344,"user":{"displayName":"김민상","userId":"05312793560890503441"},"user_tz":-540},"id":"VHcd7TCAFfxM","outputId":"1ab4de17-7d8f-495a-b2e1-4d9fa3d8c04e"},"outputs":[{"name":"stdout","output_type":"stream","text":["dialogID 4YZMOWMPX9UC\n","persona ['I would like to visit the Nazareth House again.', 'I love Benevolent institutions.', 'I am interested in History.', 'I have curiosity about the Description of this place.', 'I would like to know when it was Built.']\n","landmark_link 'https://en.wikipedia.org/wiki/Nazareth_House,_Wynnum\n","knowledge ['Nazareth House is a heritage-listed benevolent institution at 272 Wynnum North Road, Wynnum, City of Brisbane, Queensland, Australia. It was built from 1924 to 1939. It was added to the Queensland Heritage Register on 2 April 2002.', 'Nazareth House, situated on Tingal Hill, Wynnum, was designed by Brisbane architectural firm, Hennessy, Hennessy, Keesing & Co and JP Donoghue and built by George Turner. Nazareth House was officially opened by Archbishop James Duhig in 1925 as part of the charitable institution established on the site by the Poor Sisters of Nazareth.', 'A Catholic presence in Wynnum was prompted by the establishment of the area as a popular seaside resort. The opening of the Wynnum South railway station in 1898 encouraged further development, and the demand for a religious presence in the area grew. In 1903, the first Catholic masses were held in the Wynnum Shire Hall.', 'The Congregation of the Poor Sisters of Nazareth was founded in London in 1854 by Mother St Basil (born Victoire Larmenier on 21 July 1827 at Liffré, near Rennes, France). The order was established to care for the aged poor at the request of Cardinal Nicholas Wiseman, Archbishop of Westminster. The first foundation of the order to be established in Australia was at Ballarat in 1888. At the request of Archbishop Duhig, a Queensland chapter was founded in 1921. Archbishop Duhig had visited Nazareth House in London and subsequently petitioned the Mother-General to send a community of nuns to Queensland. Duhig then proceeded to search for a suitable site.', \"In 1918, Duhig purchased Mt Margaret, a property of around 60 acres (24\\xa0ha) at Tingal Hill, Wynnum, for the purposes of establishing an aged persons and children's home to be run by the Sisters of Nazareth. The property, which included a large villa, was purchased for £5000 from the trustees of the estate of the late William Kidston, Premier of Queensland from 1906-07. Kidston had entered into an agreement with Duhig prior to his death for the sale of his property for the purposes of establishing a home for the needy.\", 'Due to the difficulties of transport after World War I, it was not possible to secure passages for the sisters until 15 January 1921. On this date, six women, Sister Francis Borgia, Sister St David, Sister M Fulgentia, Sister M Maelisa, Sister Patricia Columba and Sister Joseph Comgall, from Hammersmith, London were joined at Melbourne by Sister Francis Clare (Mother Superior) and Sister Mary Margaret from Ballarat. On first arriving in Brisbane on 3 March 1921, the sisters stayed for three weeks at All Hallows Convent with the Sisters of Mercy. On 4 March, they got their first glimpse of their new home - a dwelling of nine small rooms on rising ground overlooking the sea with Wynnum two miles distant on one side and Brisbane twelve miles on the other.', 'On Good Friday, 25 March 1921, the Sisters arrived to commence charity work at Wynnum. On 1 April, the first Mass was celebrated in the house by Archbishop Duhig, who also blessed the vestments and the house. The Reverend Dean Horan of Ipswich was the first resident. The sisters also took in and cared for seven elderly ladies. In 1922 the neighbouring property of Silversprings was purchased by Archbishop Duhig for £3500. The house and other buildings from Mt Margaret were moved to the new property where they were joined to the existing house to provide accommodation for the Sisters and those in need of care. By Christmas of 1922, the new home had 35 residents.', 'Within a couple of years more accommodation was urgently needed. With the help of their benefactors and supported by the Mother House in London, the task of building a new home was undertaken. Work began on the first section in 1924 with the firm Hennessy, Hennessy, Keesing & Co and JP Donoghue, engaged by Archbishop Duhig, designing the new building. The tender of builder George Turner was accepted for £44,200. The foundation stone of what is now the eastern wing of the present building was laid by Archbishop Duhig on 13 April 1924. The building was blessed on 15 August 1925 by Archbishop Duhig and officially opened by then Governor of Queensland, Sir Matthew Nathan, the following day. £3000 was subscribed towards the cost of the new building.', \"In April 1926, the first children arrived from England to be cared for by the Sisters of Nazareth House. Local children were admitted in October the same year. The Sisters' objective was to care for orphaned or abandoned children. According to the Sisters of Nazareth House, children cared for at Wynnum during the succeeding forty years included both indigenous and non-indigenous children. Between 1889-1980 some 4000 children including orphans, wards of the State and migrant children were cared for by the Sisters of Nazareth in 8 homes across Australia and New Zealand.\", 'The Catholic Advocate in January 1930 described Nazareth House as a \"stately and impressive brick building...terraced lawns leading up to the front entrance and a fine group of statutory in the drive outside...Wide cloistered balconies surround the new building...The entrance hall has richly coloured leadlight windows and in the sitting-room and dining-room on either side are pictures of Nazareth House, Hammersmith, London, where the Sisters had their training...The pretty chapel in the left wing opens into the Sisters\" choir room...\"', 'The second stage of construction of the main building was commenced in January 1938 with the foundation stone laid by Archbishop Duhig on 20 March. The opening was held on 2 July 1939. Comprising a convent, chapel and laundry, this section was completed in April 1939 at a cost of £30,000. According to a contemporary article in the Catholic Leader, the extensions consisted of \"one million bricks.\"', 'In 1963, a brick building was constructed in the grounds to provide modern amenities for male residents and a new kitchen block. Two thirds of the cost of the extension was contributed by the Commonwealth Government. At the time of its official opening by Archbishop Duhig, Nazareth House was providing accommodation for 85 \"senior citizens\" and 70 children.', \"In 1982 Nazareth House ceased its function as a care facility for children with the move towards placing children in need with foster families. The need for a nursing home unit had long been required and it was decided that a new Nursing Home should be built on the area at the rear of the main building where the children's playground had been, and that the original building should be renovated to make single room accommodation, with ensuite facilities, for hostel residents.\", 'The new Nursing Home was blessed by the Most Reverend James Cuskelly, Auxiliary Bishop of Brisbane and opened by Senator, The Hon. Don Grimes, Minister for Social Security, on Sunday 17 June 1984. The new complex connected to the old building via covered passageways and provided four 4-bed wards, seven 2-bed wards and five single rooms. Work on renovation of the old building commenced in February 1985 and comprised the construction of 42 single rooms with ensuites.', 'Nazareth House continues to operate as an aged care facility.', \"Nazareth House is prominently located on an elevation along Wynnum North Road. The complex consists of a number of buildings including the original building (St Mary's), the Convent and Chapel and two more recent additions, St Joseph's Hostel (1960s) and the nursing home known as Larmeniere (1980s). Nazareth House has sweeping views across the north-east to Moreton Bay.\", 'St Mary\\'s is an H-shaped two storey brick building with basement and a terracotta tiled roof. Verandahs, some of which have been enclosed along the northern and eastern facades on both the ground and first floors, surround the building. A centrally located projecting entrance is situated along the northern facade of the building. The central section of the entrance rises above the roof line forming a parapet. A recess, in which sits a large rendered statue, is located in the upper section of the facade. The statue and recess are surrounded by rendered high relief moulding. Below the statue are the words \"NAZARETH HOUSE\". The parapet is surmounted by a rendered Celtic cross and rendered pedestal. Decorative relief work is located along the parapet and on the piers on either side of the central section. The first floor balcony, where it forms part of the projecting entrance, has a large, rounded arch with a moulded keystone. The ground floor verandah, similarly to the rest of the building, has decorative blue brickwork, including blue brick crosses centrally located along the entire facades.', \"On the northern facade of St Mary's, the western and eastern end of the projecting wings have parapets surmounted by rendered Celtic crosses on a rendered pedestal. This facade as the same decorative blue brickwork and decorative relief work on the underside of the parapet. The western facade, the former rear entrance, is now the main entrance to the building. The gabled parapet has a row of dentils on the underside and a group of five windows, four are narrow and rectangular in shape and the central window is larger with a round arch opening with moulded decorative detail. The parapet is surmounted by a rendered pedestal and Celtic cross. The verandahs on the ground and first floors have been enclosed and the original doorway has been widened and replaced with automatic doors. A number of other paired timber doors are located along the verandahs on both the ground and first floors. The doors are panelled and have glass breezeway and fanlight assemblies. The foundation stone is located near the front entrance along the northern facade of the building.\", \"Internally, the ground and first floors of St Mary's are similar in plan. The entrance is made up of an elaborate set of paired timber, panelled doors with large fanlight and breezeway assemblies which contain coloured glass and leadlighting. The front entrance of St Mary's, along the northern facade opens to an entry foyer with a pressed metal ceiling and timber panelled doorway and architraves. Rooms open on either side of the entrance foyer. A corridor runs east–west through the building. A turning, timber staircase with an ornate timber newel and timber rails is located off the corridor on the southern side of the central wing of the building. Rooms for the residents are located on both floors. Rooms containing offices open off the corridor. Pressed metal ceilings remain in many of the rooms including the dining room located in the south-western wing and the hall in the north-western wing. A staircase at the eastern end of the building has been isolated to form a fire stair.\", 'The convent is a U-shaped two-storey brick building with a basement level. The building has a broken-back roof clad with terracotta tiles. The building has a centrally located projecting entrance with a gabled roof, along the northern elevation. The central section of the entrance rises above the roof line forming a parapet. The parapet is surmounted by a rendered Celtic cross. Decorative relief work is located along the parapet and on the piers on either side of the central section. Paired round arched openings are located in the upper section of the facade. The openings are surrounded by decorative blue brickwork. The convent has verandahs to the ground and first floors and, similarly to the projecting entrance, the orange brickwork is surrounded by decorative blue brickwork. Along the ground level verandah has rounded arches and the first floor verandah squared arches. Double hung sash windows, some of which have been screened, are located along the entire facades of the convent.', \"Internally, the entrance foyer has a pressed metal ceiling and parquetry floor. Large double, timber, panelled doors with coloured glass and leadlighting with breezeway assembly, open to the central corridor. The hallway is rendered with decorative moulded detail along the wall. The Sister's rooms are simple in plan and decorative detail. The rooms have timber doors and architraves with breezeway assembly, painted walls and austere ceilings. Some are slightly larger than others. Hallways leading to bathroom facilities, along with the bathrooms, have terrazzo floors. The modern kitchen is located on the ground floor.\", \"The chapel is built of brick with a gabled roof at the northern end and a clad with terracotta tiles. The entrance to the chapel is located in the southern facade. Also located in this facade is the chapel's foundation stone. The gabled parapet has decorative relief work the underside and a group of three round arched windows, A smaller, narrow, rectangular window with louvres is located above these. The parapet is surmounted by a rendered Celtic cross. The chapel is fitted with copper gutters and downpipes.\", 'Internally, the chapel is rendered with a marble altar at the northern end of the building. A number of stained glass windows, as well as timber pews, are located in the chapel. A window opens in the upper section of the west wall of the chapel, this same window forms part of the convent wall. From this window, Sisters within the convent are able to have a view of the altar.', 'A rendered statue on a concrete pedestal, depicting the Holy Family, the gift of Thomas Mahon and a grotto are located in front of the northern elevation of St Mary\\'s. The entrance gates are brick with a metal archway. The archway contains the words \"NAZARETH HOUSE\" spelt out with original metal lettering.', \"The later buildings, St Josephs and Nursing Hostel, are single storey brick buildings and are located to the south of St Mary's and the convent and chapel. Covered walkways join St Mary's with both buildings. Other buildings which form part of the complex include plant rooms, a laundry and a change room.\", 'Nazareth House was listed on the Queensland Heritage Register on 2 April 2002 having satisfied the following criteria.', \"The place is important in demonstrating the evolution or pattern of Queensland's history.\", \"Nazareth House is significant for its association with the establishment of the Poor Sisters of Nazareth in Queensland and the religious and social practices they implemented. The establishment of Nazareth House reflects particularly the active interest of James Duhig and his confident building program which saw the establishment of many of the Catholic Church's prominent buildings during his time as Archbishop of Brisbane.\", 'The place is important in demonstrating the principal characteristics of a particular class of cultural places.', \"Prominently located along Wynnum North Road and visible for some distance from Wynnum, Nazareth House is significant for its considerable architectural merit, and landmark value, and is an important example of the works of one Brisbane's premier architectural practitioners and firms, Hennessy & Hennessy, Keesing and Co and JP Donoghue, who undertook many projects for the Catholic Church during Archbishop Duhig's time.\", 'The place is important because of its aesthetic significance.', \"Prominently located along Wynnum North Road and visible for some distance from Wynnum, Nazareth House is significant for its considerable architectural merit, and landmark value, and is an important example of the works of one Brisbane's premier architectural practitioners and firms, Hennessy & Hennessy, Keesing and Co and JP Donoghue, who undertook many projects for the Catholic Church during Archbishop Duhig's time.\", 'The place has a strong or special association with a particular community or cultural group for social, cultural or spiritual reasons.', 'Nazareth House is significant for its association with those, both past and present, who have lived at the house.', \"The place has a special association with the life or work of a particular person, group or organisation of importance in Queensland's history.\", \"Nazareth House is significant for its association with the establishment of the Poor Sisters of Nazareth in Queensland and the religious and social practices they implemented. The establishment of Nazareth House reflects particularly the active interest of James Duhig and his confident building program which saw the establishment of many of the Catholic Church's prominent buildings during his time as Archbishop of Brisbane.\", 'This Wikipedia article was originally based on \"The Queensland heritage register\" published by the State of Queensland under CC-BY 3.0 AU licence (accessed on 7 July 2014, archived on 8 October 2014). The geo-coordinates were originally computed from the \"Queensland heritage register boundaries\" published by the State of Queensland under CC-BY 3.0 AU licence (accessed on 5 September 2014, archived on 15 October 2014).', 'Media related to Nazareth House, Wynnum at Wikimedia Commons']\n","utterance [{'dialogue1': [\"I think I've been there before but I don't remember the name of this place.\", 'This place is the Nazareth House, which you would like to visit again.'], 'persona_grounding': [True, False, False, False, False], 'persona_candidate': ['I would like to visit the Nazareth House again.', 'I love Benevolent institutions.', 'I am interested in History.', 'I have curiosity about the Description of this place.', 'I would like to know when it was Built.'], 'knowledge_candidates': ['Nazareth House is a heritage-listed benevolent institution at 272 Wynnum North Road, Wynnum, City of Brisbane, Queensland, Australia.', 'However, in many cases, a hearing is not held.', 'The church and school buildings are listed together as a Cleveland Designated Landmark.', \"Until the reorganisation of London's local government in 1965, Muswell Hill formed part of the Borough of Hornsey within the administrative county of Middlesex.\", 'This operation enabled the Canadian Sulpicians to expand their primary work, the education of priests.', \"Bosworth's design was heavily Greek-influenced: though the facade is made of white Vermont granite, it features layers of gray granite columns in Doric and Ionic styles, as well as various Greek-inspired ornamentation.\", 'The Insurance Hall is designated as a Grade II listed building, in part due to these murals.', 'It has been pointed out that this need could have been met with the man-made Stagnum (lake) of Agrippa or, more likely, the Euripus (canal) which allowed for runoff from the Stagnum to flow into the Tiber (please see below for more information on both the Stagnum and the Euripus).', 'By 1217, documents show that the castle at Almeida is one of several strong points that guard the border between Spain and Portugal.', 'The Riverwalk runs along much of the Brisbane River foreshore throughout the inner-city area, with the longest span running between Newstead and Toowong.'], 'knowledge_answer_index': 0}, {'dialogue2': [\"I think I've been there before but I don't remember the name of this place.\", 'This place is the Nazareth House, which you would like to visit again.', 'Can you describe this house to me?', \"You have curiosity about the description of Nazareth House and I will tell you. Nazareth House is prominently located on an elevation along Wynnum North Road. The complex consists of a number of buildings including the original building, the Convent and Chapel and two more recent additions, St Joseph's Hostel and the nursing home known as Larmeniere.\"], 'persona_grounding': [False, False, False, True, False], 'persona_candidate': ['I would like to visit the Nazareth House again.', 'I love Benevolent institutions.', 'I am interested in History.', 'I have curiosity about the Description of this place.', 'I would like to know when it was Built.'], 'knowledge_candidates': ['The walls are of gray Aswan granite, carved with characters from 120 human scripts. The stated aims of the museum are to preserve heritage, foster human cadres in the conservation and restoration of manuscripts, and create a generation of new restorers.', 'Paving work has been completed at the Juniper Multi-use Area, with the addition of utilities pending. Four groups of veterans, two groups of African Americans, and one junior group, took part in various construction projects, starting with the construction of Park Road 5, a two-lane road from the rim to the floor of the canyon.', 'Because of the importance of findings in Vučedol, the whole local phase of Eneolithic period was named after it – the Vučedol Culture. [citation needed] No final conclusions about the linguistic character of Vučedol can be made, such as the inference that its people were linguistically Indo-European, or to what extent they mixed with native European populations, in regions of the eastern Adriatic coast, Dalmatia and Herzegovina with some parts of Bosnia as well.', \"Nazareth House is prominently located on an elevation along Wynnum North Road. The complex consists of a number of buildings including the original building (St Mary's), the Convent and Chapel and two more recent additions, St Joseph's Hostel (1960s) and the nursing home known as Larmeniere (1980s).\", 'A large number of these dedicated plots were established, ranging from Chelsea Pensioners and the Ancient Order of Foresters to the Corps of Commissionaires and the LSWR. Although the LNC was never able to gain the domination of London\\'s funeral industry for which its founders had hoped, it was very successful at targeting specialist groups of artisans and trades, to the extent that it became nicknamed \"the Westminster Abbey of the middle classes\".', 'In the 1920s and the 1930s it was proposed that the Mullum Mullum Valley should become protected bush parkland, but the remnant bushlands were not threatened by development, the scenery was not spectacular and society at the time had other priorities. The Mullum Mullum Creek Trail runs through much of this parkland giving cyclists through access to the entire valley.', \"The Krol group of rocks, comprising slates, marls, sandstones, limestones and dolomites with a few small dykes intrusives, is the dominant geological formation of the lake's surroundings. A study of the risk assessment code has revealed that 4–13% of manganese, 4–8% of copper, 17–24% of nickel, 3–5% of chromium, 13–26% of lead, 14–23% of cadmium and 2–3% of zinc exist in exchangeable fraction which puts the lake under the low to medium risk category and infers that it could enter into food chain and also cause deleterious effects to aquatic life.\", \"In 2014, Glasgow club Celtic played two qualifying matches at the stadium due to Celtic Park being unavailable because of Glasgow's hosting of the 2014 Commonwealth Games. It has a seating capacity of 67,144 making it the largest stadium in Scotland and the fifth largest in the United Kingdom.\", 'The chapels next to the Angel Choir were built in the Perpendicular style, with an emphasis on strong vertical lines, which survive today in the window tracery and wall panelling. Notable organists have included the Renaissance composers William Byrd and John Reading and the biographer of Mendelssohn, William Thomas Freemantle.', 'President Lyndon B. Johnson delivered a campaign address on October 27, 1964, and Sen. Barry Goldwater on October 29, 1964. The Penguins franchise agreed to a deal with city and state officials to fund a new home arena for the franchise in March 2007.'], 'knowledge_answer_index': 3}, {'dialogue3': [\"I think I've been there before but I don't remember the name of this place.\", 'This place is the Nazareth House, which you would like to visit again.', 'Can you describe this house to me?', \"You have curiosity about the description of Nazareth House and I will tell you. Nazareth House is prominently located on an elevation along Wynnum North Road. The complex consists of a number of buildings including the original building, the Convent and Chapel and two more recent additions, St Joseph's Hostel and the nursing home known as Larmeniere.\", 'Does this house look old to me, when it was built?', 'This house is relatively old, but since you would like to know when it was built, I will explain it to you. Nazareth House was built from 1924 to 1939.'], 'persona_grounding': [False, False, False, False, True], 'persona_candidate': ['I would like to visit the Nazareth House again.', 'I love Benevolent institutions.', 'I am interested in History.', 'I have curiosity about the Description of this place.', 'I would like to know when it was Built.'], 'knowledge_candidates': ['It was built from 1924 to 1939.', 'The grounds were beautiful and beautifully kept.', 'Sage Vilwamangalam once visited this temple and on entering the compound he had a vision of Lord Hanuman sitting on the branch of the huge Ilanji (Mimusops elengi) tree in the north- eastern side of the outer courtyard.', 'Frederick Handley Page was a lecturer in aeronautics at the institute.', \"30 St Mary Axe (known previously as the Swiss Re Building), informally known as The Gherkin, is a commercial skyscraper in London's primary financial district, the City of London.\", 'Eastman School of Music Campus\\nHousing is provided at the Eastman School of Music campus at the Eastman Student Living Center at 100 Gibbs Street in downtown Rochester.', 'There are various plaques on the walls of the building commemorating events in Manitoba history.', 'Her primary mission was to provide seagoing artillery support by bombarding enemy targets in the Chaho-Tanchon area, at Chongjin, in the Tanchon-Sonjin area, and at Chaho, Wonsan, Hamhung, and Hungnam during the period 25 October through 2 January 1953.', '(p217) Goat Rocks dome was the site of the bulge in the 1980 eruption, and it was obliterated in the major eruption event on May 18, 1980 that destroyed the entire north face and top 1,300 feet (400\\xa0m) of the mountain.', 'The Museu del Disseny de Barcelona (Catalan, English: \"Barcelona Design Museum\"), is a new center of Barcelona\\'s Institute of Culture, which works to promote better understanding and good use of the design world, acting as a museum and laboratory.'], 'knowledge_answer_index': 0}, {'dialogue4': [\"I think I've been there before but I don't remember the name of this place.\", 'This place is the Nazareth House, which you would like to visit again.', 'Can you describe this house to me?', \"You have curiosity about the description of Nazareth House and I will tell you. Nazareth House is prominently located on an elevation along Wynnum North Road. The complex consists of a number of buildings including the original building, the Convent and Chapel and two more recent additions, St Joseph's Hostel and the nursing home known as Larmeniere.\", 'Does this house look old to me, when it was built?', 'This house is relatively old, but since you would like to know when it was built, I will explain it to you. Nazareth House was built from 1924 to 1939.', 'What is the history of this house?', 'The history of the house you are interested in began in 1925 when it was opened by Archbishop James Duhig as part of the charity established on the site by the Poor Sisters of Nazareth. The Nazareth House, located in Tingal Hill, Wynnum, was designed by the Brisbane architecture firm, Hennessy, Hennessy, Keesing & Co and JP Donoghue and built by George Turner.'], 'persona_grounding': [False, False, True, False, False], 'persona_candidate': ['I would like to visit the Nazareth House again.', 'I love Benevolent institutions.', 'I am interested in History.', 'I have curiosity about the Description of this place.', 'I would like to know when it was Built.'], 'knowledge_candidates': ['Benfica, who included Eusébio in the team. The Riverside Stand backs onto the River Thames and is elevated above pitch level, unlike the other three stands.', 'Despite having been built during the Great Depression, the building was profitable enough that it broke even by 1936, with 90% of the space occupied five years later. The building contains numerous setbacks on its exterior.', \"When the Prince of Wales, Edward VII, visited the hotel in 1860, the commercial appeal of the adjacent neighborhood was greatly increased. On October 17, 1966, the street was the location of New York's deadliest fire until the September 11 attacks, in terms of firefighters killed.\", 'Nazareth House, situated on Tingal Hill, Wynnum, was designed by Brisbane architectural firm, Hennessy, Hennessy, Keesing & Co and JP Donoghue and built by George Turner. Nazareth House was officially opened by Archbishop James Duhig in 1925 as part of the charitable institution established on the site by the Poor Sisters of Nazareth.', 'The railway developed a network of horse-hauled road services, including providing a link between Corris station and Abergynolwyn station on the Talyllyn Railway. 8 (used as a greenhouse-cum-garden shed) was recovered in 1958 and rebuilt for use on the Talyllyn Railway as their No.17 while No.7 (used as a chicken coop) was recovered ten years later and is on display in the Corris Railway Museum.', 'Before the 18th century, the San Juan Creek watershed was mostly Acjachemem Native American territory, which extended from Aliso Creek in the north to San Mateo Creek in the south, a distance of roughly 35 miles (56\\xa0km). Oso Creek is the most heavily modified, flowing in an artificial channel for almost its entire length.', \"The buildings continued to deteriorate, and in 2004 a section of State Street was closed since part of the Wellington's cornice was in danger of falling into it. There are no significant parks or squares within it.\", 'Because the supply of AstroTurf was still low, only a limited amount was available at the start of the 1966 season. Mickey Mantle had both the first hit (a single) and the first home run in the Astrodome.', \"On 24 October 2006, Laing O'Rourke won the contract to develop the 30,000 seat stadium, which Ridsdale stated would be ready for December 2008. It also hosted the home matches of the Cardiff Blues rugby union team until the 2011–12 season, although originally the Blues had a lease until 2029.\", 'This company also includes the core operators behind Paradise Entertainment, a company which has organized events such as the Molson Indy opening ceremonies, Mid Autumn Festival 1995 and Chinese New Year celebrations in 1998 at the Plaza of Nations in Vancouver. Previously, Raymond Cheung of Target Event Production Ltd. was hoping to continue his night market at a different location in 2009 and had filed a lawsuit against Paul Cheung (no relation) of Lions Communication Inc., alleging trademark infringement over the name of the night market and copyright infringement on the vendor application forms.'], 'knowledge_answer_index': 3}]\n"]}],"source":["for i, line in enumerate(valid_focus['data']):\n","  for key in line.keys():\n","    print (key, line[key])\n","  break"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BDWKQoofK71I"},"outputs":[],"source":["# persona_grounding [True, False, False, False]\n","# knowledge_answer_index 0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8K6L6CvtF_51"},"outputs":[],"source":["import json\n","import logging\n","import os\n","import torch\n","from transformers import cached_path\n","\n","logger = logging.getLogger(__file__)\n","\n","def get_dataset_only_train_dev(tokenizer, train_dataset_path, train_dataset_cache, dev_dataset_path, dev_dataset_cache):\n","\n","    def tokenize(obj):\n","        if isinstance(obj, str):\n","            return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(obj))\n","        if isinstance(obj, dict):\n","            return dict((n, tokenize(o)) for n, o in obj.items())\n","        return list(tokenize(o) for o in obj)\n","\n","    train_dataset_cache = train_dataset_cache + '_train_focus_' + type(tokenizer).__name__\n","    dev_dataset_cache = dev_dataset_cache + '_dev_focus_' + type(tokenizer).__name__\n","\n","    # train_dataset_cache = \"\"\n","    # dev_dataset_cache = \"\"\n","    if train_dataset_cache and os.path.isfile(train_dataset_cache):\n","        logger.info(\"Load tokenized dataset from cache at %s\", train_dataset_cache)\n","        train_dataset = torch.load(train_dataset_cache)\n","        dev_dataset = torch.load(dev_dataset_cache)\n","        all_dataset = dict()\n","        all_dataset[\"train\"] = train_dataset[\"train\"]\n","        all_dataset[\"valid\"] = dev_dataset[\"valid\"]\n","    else:\n","        logger.info(\"Process dataset from %s\", train_dataset_path)\n","        plan_file_train = cached_path(train_dataset_path)\n","        plan_file_dev = cached_path(dev_dataset_path)\n","        file_dict = {\"train\": plan_file_train, \"valid\": plan_file_dev}\n","        all_dataset = dict()\n","\n","        # import pdb; pdb.set_trace()\n","\n","        for name, file in file_dict.items():\n","            # import pdb; pdb.set_trace()\n","            with open(file, \"r\", encoding=\"utf-8\") as f:\n","\n","                # import pdb;\n","                # pdb.set_trace()\n","                dataset_enc = dict()\n","                dataset_enc[name] = list()\n","                for line in f:\n","                    dialogue = json.loads(line)\n","\n","                    # import pdb;\n","                    # pdb.set_trace()\n","                    ID = dialogue[\"dialogID\"]\n","                    persona = dialogue[\"persona\"]\n","                    knowledge = dialogue[\"knowledge\"]\n","                    utterance = dialogue[\"utterance\"]\n","                    new_dialogue = dict()\n","                    new_dialogue[\"utterance\"] = list()\n","                    for i, utt in enumerate(utterance):\n","                        # import pdb; pdb.set_trace()\n","                        key = \"dialogue\" + str(i+1)\n","                        dial = utt[key]\n","                        dial_new = dict()\n","                        persona_can = utt[\"persona_candidate\"]\n","                        if len(persona_can) > 5:\n","                            persona_can = persona_can[:5]\n","                        persona_ground = utt[\"persona_grounding\"]\n","                        if len(persona_ground) > 5:\n","                            persona_ground = persona_ground[:5]\n","                        knowledge_can = utt[\"knowledge_candidates\"]\n","                        knowledge_answer = utt[\"knowledge_answer_index\"]\n","                        filtered_triple_can = utt['filtered_triple_candidates']\n","                        ners = utt['entities_in_kg']\n","\n","                        dial_enc = [tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentence.strip())) for sentence in dial]\n","                        persona_ground_enc = [1 if item==True else 0 for item in persona_ground]\n","                        knowledge_can_enc = [tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentence.strip())) for sentence in knowledge_can]\n","                        persona_can_enc = [tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentence.strip())) for sentence in persona_can]\n","\n","                        dial_new[\"dialog\"] = dial_enc\n","                        dial_new[\"persona_grounding\"] = persona_ground_enc\n","                        dial_new[\"persona_candidates\"] = persona_can_enc\n","                        dial_new[\"knowledge_candidates\"] = knowledge_can_enc\n","                        dial_new[\"knowledge_answer_index\"] = knowledge_answer\n","                        dial_new['filtered_triple_candidates'] = filtered_triple_can_enc\n","                        dial_new['ner_knowledge_candidates'] = ners_concat_ent\n","                        new_dialogue[\"utterance\"].append(dial_new)\n","\n","                        # import pdb;\n","                        # pdb.set_trace()\n","                    persona_enc = [tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentence.strip())) for sentence in persona]\n","                    knowledge_enc = [tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentence.strip())) for sentence in knowledge]\n","                    new_dialogue[\"persona\"] = persona_enc\n","                    new_dialogue[\"knowledge\"] = knowledge_enc\n","                    new_dialogue[\"dialogID\"] = ID\n","                    dataset_enc[name].append(new_dialogue)\n","            logger.info(\"Tokenize and encode the dataset\")\n","            dataset = dataset_enc\n","            all_dataset[name] = dataset_enc[name]\n","            if name == 'train':\n","                torch.save(dataset, train_dataset_cache)\n","            else:\n","                torch.save(dataset, dev_dataset_cache)\n","\n","    return all_dataset\n","\n","\n","\n","def get_dataset_only_test(tokenizer, dataset_path, dataset_cache, without_rel, line_proc):\n","\n","    def tokenize(obj):\n","        if isinstance(obj, str):\n","            return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(obj))\n","        if isinstance(obj, dict):\n","            return dict((n, tokenize(o)) for n, o in obj.items())\n","        return list(tokenize(o) for o in obj)\n","\n","    dataset_cache = dataset_cache + '_test_focus_' + type(tokenizer).__name__\n","\n","    if dataset_cache and os.path.isfile(dataset_cache):\n","        logger.info(\"Load tokenized dataset from cache at %s\", dataset_cache)\n","        dataset = torch.load(dataset_cache)\n","    else:\n","        logger.info(\"Process dataset from %s\", dataset_path)\n","        plan_file = cached_path(dataset_path)\n","        with open(plan_file, \"r\", encoding=\"utf-8\") as f:\n","            if not line_proc:\n","                dataset = json.loads(f.read())\n","            else:\n","                lines = f.readlines()\n","                dataset = [json.loads(line) for line in lines]\n","            # import pdb; pdb.set_trace()\n","            dataset_enc = dict()\n","            dataset_enc[\"test\"] = list()\n","            for dialogue in dataset:\n","                # import pdb;\n","                # pdb.set_trace()\n","                ID = dialogue[\"dialogID\"]\n","                persona = dialogue.get(\"persona\")\n","                knowledge = dialogue[\"knowledge\"]\n","                utterance = dialogue[\"utterance\"]\n","                new_dialogue = dict()\n","                new_dialogue[\"utterance\"] = list()\n","                for i, utt in enumerate(utterance):\n","                    key = \"dialogue\" + str(i+1)\n","                    dial = utt[key]\n","                    dial_new = dict()\n","                    persona_can = utt[\"persona_candidate\"]\n","                    if len(persona_can) > 5:\n","                        persona_can = persona_can[:5]\n","                    persona_ground = utt[\"persona_grounding\"]\n","                    if len(persona_ground) > 5:\n","                        persona_ground = persona_ground[:5]\n","                    knowledge_can = utt[\"knowledge_candidates\"]\n","                    knowledge_answer = utt[\"knowledge_answer_index\"]\n","                    filtered_triple_can = utt['filtered_triple_candidates']\n","                    ners = utt['entities_in_kg']\n","                    # import pdb; pdb.set_trace()\n","\n","                    dial_enc = [tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentence.strip())) for sentence in dial]\n","                    persona_ground_enc = [1 if item==True else 0 for item in persona_ground]\n","                    knowledge_can_enc = [tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentence.strip())) for sentence in knowledge_can]\n","                    persona_can_enc = [tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentence.strip())) for sentence in persona_can]\n","                    # import pdb; pdb.set_trace()\n","\n","                    # Not needed for v5, v6\n","                    # filtered_triple_can = [triple_list[0] for triple_list in filtered_triple_can ]\n","                    filtered_concat_triple_can = []\n","\n","                    filtered_triple_lists = []\n","\n","                    # assert len(filtered_triple_can) == 0 or len(filtered_triple_can) == 1\n","                    for cans in filtered_triple_can:\n","                        subjects = []\n","                        relations = []\n","                        objects = []\n","                        for can in cans:\n","                            splits = can.split(\" [SEP] \")\n","                            if len(splits) == 3:\n","                                subjects.append(splits[0])\n","                                relations.append(splits[1])\n","                                objects.append(splits[2])\n","                        if not without_rel:\n","                            sents = [subjects[i] + \" \" + relations[i] + \" \" + objects[i] for i in range(len(subjects))]\n","                            triples = [[subjects[i] + \" [SEP] \" + relations[i] + \" [SEP] \" + objects[i]] for i in range(len(subjects))]\n","                        else:\n","                            sents = [subjects[i] + \" \" + objects[i] for i in range(len(subjects))]\n","                            triples = [subjects[i] + \" [SEP] \" + objects[i] for i in range(len(subjects))]\n","                        filtered_concat_triple_can.append(\" \".join(sents))\n","                        filtered_triple_lists.append(\" [TRIPLE_SEP] \".join(triples))\n","                    filtered_triple_can_enc = [tokenizer.convert_tokens_to_ids([tok for tok in tokenizer.tokenize(\n","                        sentence.strip() if sentence else \"\")]) for sentence in filtered_concat_triple_can]\n","                    filtered_triple_lists_enc = [tokenizer.convert_tokens_to_ids([tok for tok in tokenizer.tokenize(\n","                        triple.lower().strip() if triple else \"\")]) for triple in filtered_triple_lists]\n","                    # import pdb; pdb.set_trace()\n","\n","                    ners = [ent[0] for ent in ners]\n","                    ners_concat_ent = []\n","                    for ent in ners:\n","                        rep = \"\"\n","                        # TODO : value type wise filtering\n","                        rep += \" \".join(ent.keys())\n","                        ners_concat_ent.append(rep)\n","                    ners_concat_ent = [tokenizer.convert_tokens_to_ids([tok for tok in tokenizer.tokenize(\n","                        sentence.strip() if sentence else \"\")]) for sentence in ners_concat_ent]\n","                    # import pdb; pdb.set_trace()\n","\n","                    dial_new[\"dialog\"] = dial_enc\n","                    dial_new[\"persona_candidates\"] = persona_can_enc\n","                    dial_new[\"persona_grounding\"] = persona_ground_enc\n","                    dial_new[\"knowledge_candidates\"] = knowledge_can_enc\n","                    dial_new[\"knowledge_answer_index\"] = knowledge_answer\n","                    dial_new['filtered_triple_candidates'] = filtered_triple_can_enc\n","                    dial_new['filtered_triple_candidates_raw'] = filtered_triple_lists_enc\n","                    dial_new['ner_knowledge_candidates'] = ners_concat_ent\n","                    new_dialogue[\"utterance\"].append(dial_new)\n","                persona_enc = [tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentence.strip())) for sentence in (persona if persona else persona_can)]\n","                knowledge_enc = [tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentence.strip())) for sentence in knowledge]\n","                new_dialogue[\"persona\"] = persona_enc\n","                new_dialogue[\"knowledge\"] = knowledge_enc\n","                new_dialogue[\"dialogID\"] = ID\n","                dataset_enc[\"test\"].append(new_dialogue)\n","                # import pdb; pdb.set_trace()\n","        logger.info(\"Tokenize and encode the dataset\")\n","        dataset = dataset_enc\n","        torch.save(dataset, dataset_cache)\n","\n","    return dataset\n","\n","\n","\n","class AttrDict(dict):\n","    def __init__(self, *args, **kwargs):\n","        super(AttrDict, self).__init__(*args, **kwargs)\n","        self.__dict__ = self\n","\n","\n","\n","def make_focus_logdir(dir_name: str):\n","    logdir = os.path.join(\n","        './models', dir_name\n","    )\n","    return \n"]},{"cell_type":"code","execution_count":38,"metadata":{"executionInfo":{"elapsed":851,"status":"ok","timestamp":1667289430852,"user":{"displayName":"김민상","userId":"05312793560890503441"},"user_tz":-540},"id":"LFfOZNL7MX50"},"outputs":[],"source":["from transformers import BertTokenizer"]},{"cell_type":"code","execution_count":54,"metadata":{"executionInfo":{"elapsed":465,"status":"ok","timestamp":1667289984097,"user":{"displayName":"김민상","userId":"05312793560890503441"},"user_tz":-540},"id":"nvPXnvNzMOpb"},"outputs":[],"source":["tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"]},{"cell_type":"code","execution_count":55,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1667289984098,"user":{"displayName":"김민상","userId":"05312793560890503441"},"user_tz":-540},"id":"5KWRsFYQMhbr","outputId":"e1fa2c2c-454b-4465-f3fc-e3924e432ade"},"outputs":[{"data":{"text/plain":["PreTrainedTokenizer(name_or_path='bert-base-uncased', vocab_size=30522, model_max_len=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"]},"execution_count":55,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer"]},{"cell_type":"code","execution_count":56,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":358,"status":"ok","timestamp":1667289990313,"user":{"displayName":"김민상","userId":"05312793560890503441"},"user_tz":-540},"id":"o5N9_QbaOi4I","outputId":"3e06da09-f40d-4b23-e921-ead46001e9a9"},"outputs":[{"data":{"text/plain":["30522"]},"execution_count":56,"metadata":{},"output_type":"execute_result"}],"source":["len(tokenizer)"]},{"cell_type":"code","execution_count":56,"metadata":{"executionInfo":{"elapsed":314,"status":"ok","timestamp":1667289993251,"user":{"displayName":"김민상","userId":"05312793560890503441"},"user_tz":-540},"id":"eZETScaSOkP7"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":57,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":281,"status":"ok","timestamp":1667289994788,"user":{"displayName":"김민상","userId":"05312793560890503441"},"user_tz":-540},"id":"yO4uHzlpMnRq","outputId":"3c4bddd6-c789-4d05-a24b-3c2cfae2f34d"},"outputs":[{"data":{"text/plain":["1"]},"execution_count":57,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer.add_tokens('<hello.')"]},{"cell_type":"code","execution_count":58,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":362,"status":"ok","timestamp":1667290000273,"user":{"displayName":"김민상","userId":"05312793560890503441"},"user_tz":-540},"id":"zkh178VROlQW","outputId":"5a55cf09-42d6-4e2d-aaee-0dcc851f2fb6"},"outputs":[{"data":{"text/plain":["30523"]},"execution_count":58,"metadata":{},"output_type":"execute_result"}],"source":["len(tokenizer)"]},{"cell_type":"code","execution_count":60,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"elapsed":499,"status":"ok","timestamp":1667290007090,"user":{"displayName":"김민상","userId":"05312793560890503441"},"user_tz":-540},"id":"AgXbGaPKOOAQ","outputId":"0200d088-f9bd-4a9e-dd73-cbdfe59e9b9c"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'<hello.'"]},"execution_count":60,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer.convert_ids_to_tokens(len(tokenizer)-1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xQ-rFF04OoSa"},"outputs":[],"source":["model.resize_embedding(len(tokenizer))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_KGxxzhgxZk8"},"outputs":[],"source":["b#(c) 2021 NCSOFT Corporation & Korea University. All rights reserved.\n","import logging\n","import torch\n","import torch.nn as nn\n","from torch.nn import CrossEntropyLoss, BCEWithLogitsLoss\n","from transformers import GPT2Model, GPT2PreTrainedModel, GPT2LMHeadModel\n","from transformers import BartModel, BartPretrainedModel, BartForConditionalGeneration\n","from torch.nn import Sigmoid, Softmax\n","\n","\n","logger = logging.getLogger(__name__)\n","\n","\n","class ConcatSummary(nn.Module):\n","    def __init__(self, emb_dim=768):\n","        super().__init__()\n","        self.dropout = nn.Dropout(0.1)\n","        self.summary = nn.Linear(emb_dim * 7, 1)  # hiddensize, numclasses\n","    def forward(self, output):\n","        dropout_pooled_output = self.dropout(output)\n","        logits = self.summary(dropout_pooled_output)\n","        return logits\n","\n","class Summary(nn.Module):\n","    def __init__(self, emb_dim=768):\n","        super().__init__()\n","        self.dropout = nn.Dropout(0.1)\n","        self.summary = nn.Linear(emb_dim , 1)  # hiddensize, numclasses\n","    def forward(self, output):\n","        dropout_pooled_output = self.dropout(output)\n","        logits = self.summary(dropout_pooled_output)\n","        return logits\n","\n","\n","\n","class GPT2PK_ctxt(GPT2LMHeadModel):\n","    _keys_to_ignore_on_load_missing = [r\"h\\.\\d+\\.attn\\.masked_bias\", r\"lm_head\\.weight\"]\n","\n","    def __init__(self, config):\n","        super().__init__(config)\n","        self.transformer = GPT2Model(config)\n","        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n","        self.concat_summary = ConcatSummary(emb_dim=config.n_embd)\n","        self.summary = Summary(emb_dim=config.n_embd)\n","        self.attn1 = nn.Linear(config.n_embd, 5)\n","        self.attn2 = nn.Linear(5, config.n_embd)# Selected knowledge 개수만\n","        self.max_position = config.n_positions\n","        #self.paragraph_sum = nn.Linear(config.n_embd, 1)\n","        self.init_weights()\n","\n","    def forward(\n","            self,\n","            input_ids=None,\n","            input_eos=None,\n","            token_type_ids=None,\n","            only_dial_input_ids=None,\n","            only_dial_token_type_ids=None,\n","            persona_input_ids=None,\n","            knowledge_input_ids=None,\n","            persona_can_idx=None,\n","            persona_grounding=None,\n","            knowledge_can_idx=None,\n","            knowledge_grounding=None,\n","            tot_knowledge=None,\n","            tot_knowledge_token_ids=None,\n","            tot_knowledge_eos=None,\n","            training=None,\n","            mc_token_ids=None):\n","\n","        persona = 50259\n","        knowledge = 50260\n","        padding = 50261\n","        bos = 50256\n","        device = input_ids.get_device()\n","\n","        persona_tensor = torch.tensor([persona]).cuda(device)\n","        knowledge_tensor = torch.tensor([knowledge]).cuda(device)\n","        bos_tensor = torch.tensor([bos]).cuda(device)\n","        outputs = tuple()\n","        dynamic_lm_logits = None\n","        persona_logits = None\n","        knowledge_logits = None\n","        lm_labels=None\n","\n","        if input_eos is not None:\n","            lm_hidden_states = self.transformer(input_ids=input_ids, token_type_ids=token_type_ids)['last_hidden_state']\n","            batch, seq_len, embdim = lm_hidden_states.size()\n","            lm_hidden_states_eos_list = []\n","            for i in range(batch):\n","                lm_hidden_states_batch = lm_hidden_states[i]\n","                lm_eos_batch = input_eos[i]\n","                lm_hidden_states_eos = torch.index_select(lm_hidden_states_batch, -2, lm_eos_batch)\n","                lm_hidden_states_eos_list.append(lm_hidden_states_eos)\n","            lm_eos_rep = torch.stack(lm_hidden_states_eos_list)\n","            #print(\"lm eos rep: \", lm_eos_rep.size()) #batch, 1, embdim\n","\n","            tot_knowledge_hidden_states = self.transformer(input_ids = tot_knowledge, token_type_ids=tot_knowledge_token_ids)['last_hidden_state']\n","            #print(\"tot knowledge: \", tot_knowledge_hidden_states.size()) #batch, 5(# paragraph), seqlen, embdim\n","            tot_knowledge_eos_list = []\n","            for i in range(batch):\n","                tot_knowledge_hidden_states_batch = tot_knowledge_hidden_states[i]\n","                tot_knowledge_eos_batch = tot_knowledge_eos[i]\n","                #print(\"tot_knowledge_hid batch: \", tot_knowledge_hidden_states_batch.size(), tot_knowledge_eos_batch.size()) #5, seqlen, embdim / 5\n","                tot_knowledge_eos_list_batch = []\n","                for j in range(5):\n","                    tot_knowledge_eos_token = torch.index_select(tot_knowledge_hidden_states_batch[j], -2, tot_knowledge_eos_batch[j])\n","                    tot_knowledge_eos_list_batch.append(tot_knowledge_eos_token.squeeze())\n","                tot_knowledge_eos_batch_rep = torch.stack(tot_knowledge_eos_list_batch)\n","                tot_knowledge_eos_list.append(tot_knowledge_eos_batch_rep)\n","            tot_knowledge_eos_final = torch.stack(tot_knowledge_eos_list)\n","            knowledge_inctxt_attn = self.attn1(tot_knowledge_eos_final)\n","            knowledge_inctxt_eos_rep = self.attn2(knowledge_inctxt_attn)\n","            inctxt_states = torch.cat((lm_eos_rep, knowledge_inctxt_eos_rep), dim=1).type_as(input_ids)\n","\n","            sigmoid = Sigmoid()\n","            #persona candidates\n","            num_persona_can = 5\n","            if persona_input_ids is not None:\n","                persona_emb = self.transformer(input_ids=persona_input_ids)['last_hidden_state']\n","                if persona_can_idx is not None:\n","                    persona_list = []\n","                    for batch_i in range(batch):\n","                        inctxt_eos_batch = inctxt_states[batch_i] #6, 768\n","                        persona_emb_batch = persona_emb[batch_i]\n","                        persona_can_idx_batch = persona_can_idx[batch_i]\n","                        persona_batch_list = []\n","                        for i in range(num_persona_can):\n","                            persona_selected = torch.index_select(persona_emb_batch[i], 0, persona_can_idx_batch[i])\n","                            final_rep_persona = torch.cat([inctxt_eos_batch.type_as(lm_eos_rep), persona_selected.type_as(lm_eos_rep)], dim=0) #7,768\n","                            persona_batch_list.append(final_rep_persona)\n","                        persona_batch_list = torch.stack(persona_batch_list)\n","                        persona_list.append(persona_batch_list)\n","                    persona_rep = torch.stack(persona_list).view(batch*num_persona_can, -1)\n","                    persona_logits = self.concat_summary(persona_rep).view(batch, -1)\n","                    outputs = (persona_logits, )\n","\n","                    persona_pred_sigmoid = sigmoid(persona_logits)\n","                    persona_pred_sigmoid = (persona_pred_sigmoid > 0.5).float()\n","                    all_persona_pred = []\n","                    selected_persona_idx = list()\n","                    for batch_idx, persona_batch in enumerate(torch.eq(persona_pred_sigmoid, 1)):\n","                        batch_list_idx = list()\n","                        batch_list = list()\n","                        for i, can in enumerate(persona_batch):\n","                            if can == True:\n","                                batch_list_idx.append(can)\n","                                persona_selected_now = persona_input_ids[batch_idx][i]\n","                                mask_persona = torch.ne(persona_selected_now, padding)\n","                                persona_selected_now = torch.masked_select(persona_selected_now, mask_persona)\n","                                batch_list.append(persona_selected_now[:-2])\n","                        all_persona_pred.append(batch_list)\n","                        selected_persona_idx.append(batch_list_idx)\n","\n","\n","            #knowledge candidates\n","            num_knowledge_can = 10\n","            if knowledge_input_ids is not None:\n","                knowledge_emb = self.transformer(input_ids=knowledge_input_ids)['last_hidden_state']\n","                if knowledge_can_idx is not None:\n","                    knowledge_list = []\n","                    for batch_i in range(batch):\n","                        inctxt_eos_batch = inctxt_states[batch_i]\n","                        knowledge_emb_batch = knowledge_emb[batch_i]\n","                        knowledge_can_idx_batch = knowledge_can_idx[batch_i]\n","                        knowledge_batch_list = []\n","                        for i in range(num_knowledge_can):\n","                            knowledge_selected = torch.index_select(knowledge_emb_batch[i], 0, knowledge_can_idx_batch[i])\n","                            final_rep_knowledge = torch.cat([inctxt_eos_batch.type_as(lm_eos_rep), knowledge_selected.type_as(lm_eos_rep)], dim=0)\n","                            knowledge_batch_list.append(final_rep_knowledge)\n","                        knowledge_batch_list = torch.stack(knowledge_batch_list)\n","                        knowledge_list.append(knowledge_batch_list)\n","                    knowledge_rep = torch.stack(knowledge_list).view(batch*num_knowledge_can, -1)\n","                    knowledge_logits = self.concat_summary(knowledge_rep).view(batch, -1)\n","                    outputs = (knowledge_logits,) + outputs\n","                    softmax = Softmax(dim=-1)\n","                    knowledge_softmax = softmax(knowledge_logits)\n","                    _, k_index_1 = torch.topk(knowledge_softmax, k=1, dim=-1)\n","                    all_knowledge_pred = []\n","                    for batch_i in range(batch):\n","                        knowledge_pred_idx = k_index_1[batch_i]\n","                        knowledge_pred = knowledge_input_ids[batch_i][knowledge_pred_idx]\n","                        mask_knowledge = torch.ne(knowledge_pred, padding)\n","                        knowledge_pred = torch.masked_select(knowledge_pred, mask_knowledge)\n","                        knowledge_pred = knowledge_pred[1:-2] #delete bos, knowledge_st, eos\n","                        if knowledge_pred.size()[0] > 150:\n","                            knowledge_pred = knowledge_pred[:150]\n","                        all_knowledge_pred.append(knowledge_pred)\n","\n","\n","            final_input_list = []\n","            final_input_tti_list = []\n","            final_lm_label_list = []\n","            for batch_i in range(batch):\n","                only_dial_input_ids_batch = only_dial_input_ids[batch_i]\n","                only_dial_token_type_ids_batch = only_dial_token_type_ids[batch_i]\n","                mask_only_dial_input_ids_batch = torch.ne(only_dial_input_ids_batch, padding)\n","                mask_only_dial_tti_batch = torch.ne(only_dial_token_type_ids_batch, padding)\n","                only_dial_input_ids_batch = torch.masked_select(only_dial_input_ids_batch, mask_only_dial_input_ids_batch)\n","                only_dial_token_type_ids_batch = torch.masked_select(only_dial_token_type_ids_batch, mask_only_dial_tti_batch)\n","                if len(all_persona_pred[batch_i])>0:\n","                    concat_persona = torch.cat(all_persona_pred[batch_i], dim=-1)\n","                    new_persona = torch.cat([persona_tensor, concat_persona], dim=-1)\n","                    new_persona_tti = torch.tensor([persona] * (new_persona.size()[0])).cuda(device)\n","                else:\n","                    new_persona = None\n","                    new_persona_tti = None\n","\n","                new_knowledge = torch.cat([knowledge_tensor, all_knowledge_pred[batch_i]], dim=-1)\n","                new_knowledge_tti = torch.tensor([knowledge] * (new_knowledge.size()[0])).cuda(device)\n","\n","                if new_persona is not None:\n","                    new_input = torch.cat([bos_tensor, new_knowledge, new_persona, only_dial_input_ids_batch], dim=-1)\n","                    new_input_tti = torch.cat([knowledge_tensor, new_knowledge_tti, new_persona_tti, only_dial_token_type_ids_batch], dim=-1)\n","                else:\n","                    new_input = torch.cat([bos_tensor, new_knowledge, only_dial_input_ids_batch], dim=-1)\n","                    new_input_tti = torch.cat([knowledge_tensor, new_knowledge_tti, only_dial_token_type_ids_batch], dim=-1)\n","\n","                new_input_size = new_input.size()[0]\n","                new_lm_label = torch.cat([torch.tensor([-100] * (new_input_size-(only_dial_input_ids_batch.size()[0])+1)).cuda(device), only_dial_input_ids_batch[1:]], dim=-1)\n","                assert new_input.size() == new_input_tti.size() == new_lm_label.size()\n","                if new_input_size < int(self.max_position):\n","                    padding_size = int(self.max_position)-new_input_size\n","                    add_padding = torch.tensor([padding]*padding_size).cuda(device)\n","                    add_lm_padding = torch.tensor([-100]*padding_size).cuda(device)\n","                    final_input = torch.cat([new_input, add_padding], dim=-1)\n","                    final_tti_input = torch.cat([new_input_tti, add_padding], dim=-1)\n","                    final_lm_label = torch.cat([new_lm_label, add_lm_padding], dim=-1)\n","                final_input_list.append(final_input)\n","                final_input_tti_list.append(final_tti_input)\n","                final_lm_label_list.append(final_lm_label)\n","            input_ids = torch.stack(final_input_list)\n","            token_type_ids = torch.stack(final_input_tti_list)\n","            lm_labels = torch.stack(final_lm_label_list)\n","\n","\n","        dynamic_lm_hidden_states = self.transformer(input_ids=input_ids, token_type_ids=token_type_ids)['last_hidden_state']\n","        if dynamic_lm_hidden_states is not None:\n","            dynamic_lm_logits = self.lm_head(dynamic_lm_hidden_states)\n","            outputs = (dynamic_lm_logits,) + outputs\n","\n","        if persona_grounding is not None:\n","            loss_fct = BCEWithLogitsLoss()\n","            persona_loss = loss_fct(persona_logits.view(batch, -1), persona_grounding.type_as(persona_logits))\n","            outputs = (persona_loss,) + outputs\n","\n","        if knowledge_grounding is not None:\n","            loss_fct = CrossEntropyLoss()\n","            knowledge_loss = loss_fct(knowledge_logits.view(batch, -1), knowledge_grounding)\n","            outputs = (knowledge_loss,) + outputs\n","\n","        if training is not True:\n","            outputs = (lm_labels,) + outputs\n","            lm_labels = None\n","\n","        if lm_labels is not None:\n","            shift_logits = dynamic_lm_logits[..., :-1, :].contiguous()\n","            shift_labels = lm_labels[..., 1:].contiguous()\n","            loss_fct = CrossEntropyLoss(ignore_index=-100)\n","            lm_loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n","            outputs = (lm_loss,) + outputs\n","\n","        return outputs  # (lm_loss-training), (lm_label-validation), (knowledge_loss), (persona_loss), dynamic_lm_logits, knowledge_logits, persona_logits, presents, (all hidden_states), (attentions)\n","\n","\n","\n","class BARTPK_ctxt(BartForConditionalGeneration):\n","    _keys_to_ignore_on_load_missing = [r\"h\\.\\d+\\.attn\\.masked_bias\", r\"lm_head\\.weight\"]\n","\n","    def __init__(self, config):\n","        super().__init__(config)\n","\n","        self.model = BartModel(config)\n","        self.lm_head = nn.Linear(config.d_model, config.vocab_size, bias=False) # 768 x 32000 -> argmax -> idx -> token\n","        self.concat_summary = ConcatSummary(emb_dim=config.d_model)\n","        self.summary = Summary(emb_dim=config.d_model)\n","        self.attn1 = nn.Linear(config.d_model, 5)\n","        self.attn2 = nn.Linear(5, config.d_model)  # Selected knowledge 개수만\n","        self.max_position = config.max_position_embeddings\n","        self.init_weights()\n","\n","    def forward(\n","            self,\n","            input_ids=None,\n","            input_eos=None,\n","            only_dial_input_ids=None,\n","            decoder_input_ids=None,\n","            persona_input_ids=None,\n","            knowledge_input_ids=None,\n","            persona_can_idx=None,\n","            persona_grounding=None,\n","            knowledge_can_idx=None,\n","            knowledge_grounding=None,\n","            tot_knowledge=None,\n","            tot_knowledge_eos=None,\n","            training=None,\n","            lm_labels=None,\n","            mc_token_ids=None):\n","\n","        machine = 50265\n","        human = 50266\n","        persona = 50267\n","        knowledge = 50268\n","        padding = 1\n","        bos = 0\n","        eos = 2\n","        num_chosen_paragraph = 5\n","        device = input_ids.get_device()\n","\n","        persona_tensor = torch.tensor([persona]).cuda(device)\n","        knowledge_tensor = torch.tensor([knowledge]).cuda(device)\n","        bos_tensor = torch.tensor([bos]).cuda(device)\n","        eos_tensor = torch.tensor([eos]).cuda(device)\n","\n","        outputs = tuple()\n","        dynamic_lm_logits = None\n","        persona_logits = None\n","        knowledge_logits = None\n","\n","        # TODO : To find out - what is input_eos? Probably end of sentence for the inputs\n","        if input_eos is not None:\n","            lm_hidden_states = self.model(input_ids=input_ids)['last_hidden_state']\n","            batch, seq_len, embdim = lm_hidden_states.size()\n","            lm_hidden_states_eos_list = []\n","            for i in range(batch):\n","                lm_hidden_states_batch = lm_hidden_states[i]\n","                lm_eos_batch = input_eos[i]\n","                lm_hidden_states_eos = torch.index_select(lm_hidden_states_batch, -2, lm_eos_batch)\n","                lm_hidden_states_eos_list.append(lm_hidden_states_eos)\n","            lm_eos_rep = torch.stack(lm_hidden_states_eos_list)\n","\n","            tot_knowledge_hidden_states = self.model(input_ids=tot_knowledge.view(batch*num_chosen_paragraph, -1))['last_hidden_state'].view(batch, num_chosen_paragraph, -1, embdim)\n","            tot_knowledge_eos_list = []\n","            for i in range(batch):\n","                tot_knowledge_hidden_states_batch = tot_knowledge_hidden_states[i]\n","                tot_knowledge_eos_batch = tot_knowledge_eos[i]\n","                tot_knowledge_eos_list_batch = []\n","                # TODO : This is hard coded (5), where does this come from?\n","                for j in range(5):\n","                    tot_knowledge_eos_token = torch.index_select(tot_knowledge_hidden_states_batch[j], -2, tot_knowledge_eos_batch[j])\n","                    tot_knowledge_eos_list_batch.append(tot_knowledge_eos_token.squeeze())\n","                tot_knowledge_eos_batch_rep = torch.stack(tot_knowledge_eos_list_batch)\n","                tot_knowledge_eos_list.append(tot_knowledge_eos_batch_rep)\n","            tot_knowledge_eos_final = torch.stack(tot_knowledge_eos_list)\n","            knowledge_inctxt_attn = self.attn1(tot_knowledge_eos_final)\n","            knowledge_inctxt_eos_rep = self.attn2(knowledge_inctxt_attn)\n","            inctxt_states = torch.cat((lm_eos_rep, knowledge_inctxt_eos_rep), dim=1).type_as(input_ids)\n","\n","            sigmoid = Sigmoid()\n","\n","            #persona candidates\n","            num_persona_can = 5\n","            if persona_input_ids is not None:\n","                persona_emb = self.model(input_ids=persona_input_ids.view(batch*num_persona_can,-1))['last_hidden_state'].view(batch, num_persona_can, -1, embdim)\n","                if persona_can_idx is not None:\n","                    persona_list = []\n","                    for batch_i in range(batch):\n","                        inctxt_eos_batch = inctxt_states[batch_i]\n","                        persona_emb_batch = persona_emb[batch_i]\n","                        persona_can_idx_batch = persona_can_idx[batch_i]\n","                        persona_batch_list = []\n","                        for i in range(num_persona_can):\n","                            persona_selected = torch.index_select(persona_emb_batch[i], 0, persona_can_idx_batch[i])\n","                            final_rep_persona = torch.cat([inctxt_eos_batch.type_as(lm_eos_rep), persona_selected.type_as(lm_eos_rep)], dim=0)\n","                            persona_batch_list.append(final_rep_persona)\n","                        persona_batch_list = torch.stack(persona_batch_list)\n","                        persona_list.append(persona_batch_list)\n","                    persona_rep = torch.stack(persona_list).view(batch*num_persona_can, -1)\n","                    persona_logits = self.concat_summary(persona_rep).view(batch, -1)\n","                    outputs = (persona_logits, )\n","\n","                    persona_pred_sigmoid = sigmoid(persona_logits)\n","                    persona_pred_sigmoid = (persona_pred_sigmoid > 0.5).float()\n","                    all_persona_pred = []\n","                    selected_persona_idx = list()\n","                    for batch_idx, persona_batch in enumerate(torch.eq(persona_pred_sigmoid, 1)):\n","                        batch_list_idx = list()\n","                        batch_list = list()\n","                        for i, can in enumerate(persona_batch):\n","                            if can == True:\n","                                batch_list_idx.append(can)\n","                                persona_selected_now = persona_input_ids[batch_idx][i]\n","                                mask_persona = torch.ne(persona_selected_now, padding)\n","                                persona_selected_now = torch.masked_select(persona_selected_now, mask_persona)\n","                                batch_list.append(persona_selected_now[:-2])\n","                        all_persona_pred.append(batch_list)\n","                        selected_persona_idx.append(batch_list_idx)\n","\n","            #knowledge candidates\n","            num_knowledge_can = 10\n","            if knowledge_input_ids is not None:\n","                knowledge_emb = self.model(input_ids=knowledge_input_ids.view(batch*num_knowledge_can, -1))['last_hidden_state'].view(batch, num_knowledge_can, -1, embdim)\n","                if knowledge_can_idx is not None:\n","                    knowledge_list = []\n","                    for batch_i in range(batch):\n","                        inctxt_eos_batch = inctxt_states[batch_i]\n","                        knowledge_emb_batch = knowledge_emb[batch_i]\n","                        knowledge_can_idx_batch = knowledge_can_idx[batch_i]\n","                        knowledge_batch_list = []\n","                        for i in range(num_knowledge_can):\n","                            knowledge_selected = torch.index_select(knowledge_emb_batch[i], 0, knowledge_can_idx_batch[i])\n","                            final_rep_knowledge = torch.cat([inctxt_eos_batch.type_as(lm_eos_rep), knowledge_selected.type_as(lm_eos_rep)], dim=0)\n","                            knowledge_batch_list.append(final_rep_knowledge)\n","                        knowledge_batch_list = torch.stack(knowledge_batch_list)\n","                        knowledge_list.append(knowledge_batch_list)\n","                        # TODO : MODIFY\n","                        # import pdb; pdb.set_trace()\n","\n","                    knowledge_rep = torch.stack(knowledge_list).view(batch*num_knowledge_can, -1)\n","                    knowledge_logits = self.concat_summary(knowledge_rep).view(batch, -1)\n","                    outputs = (knowledge_logits,) + outputs\n","                    softmax = Softmax(dim=-1)\n","                    knowledge_softmax = softmax(knowledge_logits)\n","                    _, k_index_1 = torch.topk(knowledge_softmax, k=1, dim=-1)\n","                    all_knowledge_pred = []\n","                    for batch_i in range(batch):\n","                        knowledge_pred_idx = k_index_1[batch_i]\n","                        knowledge_pred = knowledge_input_ids[batch_i][knowledge_pred_idx]\n","                        mask_knowledge = torch.ne(knowledge_pred, padding)\n","                        knowledge_pred = torch.masked_select(knowledge_pred, mask_knowledge)\n","                        knowledge_pred = knowledge_pred[1:-2]\n","                        all_knowledge_pred.append(knowledge_pred) #delete bos, knowledge_st, eos\n","\n","\n","            final_input_list = []\n","            for batch_i in range(batch):\n","                only_dial_input_ids_batch = only_dial_input_ids[batch_i]\n","                mask_only_dial_input_ids_batch = torch.ne(only_dial_input_ids_batch, padding)\n","                only_dial_input_ids_batch = torch.masked_select(only_dial_input_ids_batch, mask_only_dial_input_ids_batch)\n","                if len(all_persona_pred[batch_i])>0:\n","                    concat_persona = torch.cat(all_persona_pred[batch_i], dim=-1)\n","                    new_persona = torch.cat([persona_tensor, concat_persona], dim=-1)\n","                else:\n","                    new_persona = None\n","\n","                new_knowledge = torch.cat([knowledge_tensor, all_knowledge_pred[batch_i]], dim=-1)\n","\n","                if new_persona is not None:\n","                    new_input = torch.cat([bos_tensor, new_knowledge, new_persona, only_dial_input_ids_batch, eos_tensor], dim=-1)\n","                else:\n","                    new_input = torch.cat([bos_tensor, new_knowledge, only_dial_input_ids_batch, eos_tensor], dim=-1)\n","\n","                new_input_size = new_input.size()[0]\n","\n","\n","                if new_input_size < int(self.max_position) :\n","                    padding_size = int(self.max_position) - new_input_size\n","                    add_padding = torch.tensor([padding]*padding_size).cuda(device)\n","                    final_input = torch.cat([new_input, add_padding], dim=-1)\n","                final_input_list.append(final_input)\n","            input_ids = torch.stack(final_input_list)\n","        # input : <human> Hello <machine> What can I do for you? <human> ... <persona> adlkasjdlkaj <knowledge> asldkjasldkj\n","        dynamic_lm_hidden_states = self.model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)['last_hidden_state']\n","\n","        if dynamic_lm_hidden_states is not None:\n","            dynamic_lm_logits = self.lm_head(dynamic_lm_hidden_states)\n","            outputs = (dynamic_lm_logits,) + outputs\n","\n","        if persona_grounding is not None:\n","            loss_fct = BCEWithLogitsLoss()\n","            persona_loss = loss_fct(persona_logits.view(batch, -1), persona_grounding.type_as(persona_logits))\n","            outputs = (persona_loss,) + outputs\n","\n","        if knowledge_grounding is not None:\n","            loss_fct = CrossEntropyLoss()\n","            knowledge_loss = loss_fct(knowledge_logits.view(batch, -1), knowledge_grounding)\n","            outputs = (knowledge_loss,) + outputs\n","\n","        if training is True:\n","            shift_logits = dynamic_lm_logits[..., :-1, :].contiguous() # <sos> ..... \n","            shift_labels = lm_labels[..., 1:].contiguous() # ... <eos>\n","            loss_fct = CrossEntropyLoss(ignore_index=-100) # padding, -> loss\n","            lm_loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n","            outputs = (lm_loss,) + outputs\n","\n","\n","        return "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_zQhBYI9vJiC"},"outputs":[],"source":["import os\n","import math\n","import logging\n","from pprint import pformat\n","from argparse import ArgumentParser\n","import torch\n","from torch.nn import Sigmoid, Softmax\n","from torch.nn.parallel import DistributedDataParallel\n","from ignite.engine import Engine, Events\n","from ignite.handlers import ModelCheckpoint\n","from ignite.metrics import Loss, MetricsLambda, RunningAverage, Precision, CharFbeta, Recall, Accuracy\n","from ignite.metrics import Bleu, RougeL, RougeN\n","from ignite.contrib.handlers import ProgressBar, PiecewiseLinear\n","from ignite.contrib.handlers.tensorboard_logger import TensorboardLogger, OutputHandler, OptimizerParamsHandler\n","from transformers import (AdamW, WEIGHTS_NAME, CONFIG_NAME)\n","from utils_focus import make_focus_logdir\n","from data_utils import get_data_loaders, add_special_tokens_\n","\n","logger = logging.getLogger(__file__)\n","\n","\n","def average_distributed_scalar(scalar, args):\n","    \"\"\" Average a scalar over the nodes if we are in distributed training. We use this for distributed evaluation. \"\"\"\n","    if args.local_rank == -1:\n","        return scalar\n","    scalar_t = torch.tensor(scalar, dtype=torch.float, device=args.device) / torch.distributed.get_world_size()\n","    torch.distributed.all_reduce(scalar_t, op=torch.distributed.ReduceOp.SUM)\n","    return scalar_t.item()\n","\n","\n","def train():\n","    parser = ArgumentParser()\n","    parser.add_argument(\"--model_name\", type=str, default=\"\",\n","                        help=\"{GPT2, BART, transformer-decoder, transformer-encdec}\")\n","    parser.add_argument(\"--gpt2_model_path\", type=str, default=\"gpt2\",\n","                        help=\"pre-trained model path for decoder only models\")  # gpt2-medium\n","    parser.add_argument(\"--bart_model_path\", type=str, default=\"facebook/bart-base\",\n","                        help=\"pre-trained model path for encoder-decoder models\")  # facebook/bart-large\n","    parser.add_argument(\"--train_dataset_path\", type=str, default=\"data/train_focus.json\",\n","                        help=\"Path or url of the dataset.\")\n","    parser.add_argument(\"--train_dataset_cache\", type=str, default='data/focus_cache.tar.gz',\n","                        help=\"Path or url of the dataset cache\")\n","    parser.add_argument(\"--dev_dataset_path\", type=str, default=\"data/valid_focus.json\",\n","                        help=\"Path or url of the dataset.\")\n","    parser.add_argument(\"--dev_dataset_cache\", type=str, default='data/focus_cache.tar.gz',\n","                        help=\"Path or url of the dataset cache\")\n","    parser.add_argument(\"--ps_coef\", type=float, default=1.0, help=\"Coefficient for persona loss\")\n","    parser.add_argument(\"--kn_coef\", type=float, default=1.0, help=\"Coefficient for knowledge loss\")\n","    parser.add_argument(\"--lm_coef\", type=float, default=10.0, help=\"Coefficient for LM loss\")\n","    parser.add_argument(\"--max_history\", type=int, default=1, help=\"Number of previous exchanges to keep in history\")\n","    parser.add_argument(\"--train_batch_size\", type=int, default=4, help=\"Batch size for training\")\n","    parser.add_argument(\"--valid_batch_size\", type=int, default=1, help=\"Batch size for validation\")\n","    parser.add_argument(\"--gradient_accumulation_steps\", type=int, default=16,\n","                        help=\"Accumulate gradients on several steps\")\n","    parser.add_argument(\"--lr\", type=float, default=6.25e-5, help=\"Learning rate\")\n","    parser.add_argument(\"--max_norm\", type=float, default=1.0, help=\"Clipping gradient norm\")\n","    parser.add_argument(\"--n_epochs\", type=int, default=1, help=\"Number of training epochs\")\n","    parser.add_argument(\"--eval_before_start\", action='store_true',\n","                        help=\"If true start with a first evaluation before training\")\n","    parser.add_argument(\"--inference\", action='store_true', help=\"If true, inference with gold knowledge\")\n","    parser.add_argument(\"--test_infer\", action='store_true', help=\"If true, test inference\")\n","    parser.add_argument(\"--device\", type=str, default=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n","                        help=\"Device (cuda or cpu)\")\n","    parser.add_argument(\"--fp16\", type=str, default=\"\",\n","                        help=\"Set to O0, O1, O2 or O3 for fp16 training (see apex documentation)\")\n","    parser.add_argument(\"--local_rank\", type=int, default=-1,\n","                        help=\"Local rank for distributed training (-1: not distributed)\")\n","    parser.add_argument(\"--gpu_start_num\", type=int, default=1, help=\"Start number of GPU\")\n","    parser.add_argument(\"--flag\", type=str, default=\"\", help=\"Assign the name of the folder\")\n","    parser.add_argument(\"--seed\", type=int, default=19950604)\n","    parser.add_argument(\"--random_knowledge\", action='store_true',\n","                        help=\"If true, the model choose the knowledge randomly\")\n","    parser.add_argument(\"--incontext\", action='store_true', help=\"If true, it will use incontext structure\")\n","    parser.add_argument(\"--append_k_triples\", action='store_true', help=\"If true, inference with triples instead of knowledge\")\n","    parser.add_argument(\"--replace_k_triples\", action='store_true', help=\"If true, inference with triples instead of knowledge\")\n","    args = parser.parse_args()\n","    torch.manual_seed(args.seed)\n","\n","    logging.basicConfig(level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN)\n","    logger.info(\"Arguments: %s\", pformat(args))\n","\n","    args.distributed = (args.local_rank != -1)\n","    if args.distributed:\n","        local_rank = args.local_rank + args.gpu_start_num\n","        print(\"args local rank: \", args.local_rank, \" local rank: \", local_rank)\n","        torch.cuda.set_device(local_rank)\n","        args.device = torch.device(\"cuda\", local_rank)\n","        torch.distributed.init_process_group(backend='nccl', init_method='env://')\n","\n","    logger.info(\"Prepare tokenizer, pretrained model and optimizer.\")\n","\n","    if args.model_name == 'GPT2':\n","        from transformers import GPT2Tokenizer\n","        from classification_modules import GPT2PK_ctxt as gpt2model\n","        tokenizer = GPT2Tokenizer.from_pretrained(args.gpt2_model_path)\n","        model = gpt2model.from_pretrained(args.gpt2_model_path)\n","        model.to(args.device)\n","        model.eval()\n","        if args.gpt2_model_path == 'gpt2' or 'gpt2-medium':\n","            add_special_tokens_(model, tokenizer)\n","\n","    elif args.model_name == 'BART':\n","        from transformers import BartTokenizer\n","        from classification_modules import BARTPK_ctxt as bartmodel\n","        tokenizer = BartTokenizer.from_pretrained(args.bart_model_path)\n","        model = bartmodel.from_pretrained(args.bart_model_path)\n","        model.to(args.device)\n","        model.eval()\n","        if args.bart_model_path == \"facebook/bart-base\" or \"facebook/bart-large\":\n","            add_special_tokens_(model, tokenizer)\n","\n","    elif args.model_name == 'transformer-decoder':\n","        from transformers import GPT2Tokenizer, GPT2Config\n","        from classification_modules import GPT2PK_ctxt as gpt2model\n","        tokenizer = GPT2Tokenizer.from_pretrained(args.gpt2_model_path)\n","        model_config = GPT2Config.from_pretrained(args.gpt2_model_path)\n","        model = gpt2model(model_config)\n","        model.to(args.device)\n","        if args.gpt2_model_path == 'gpt2' or 'gpt2-medium':\n","            add_special_tokens_(model, tokenizer)\n","\n","    elif args.model_name == 'transformer-encdec':\n","        from transformers import BartTokenizer, BartConfig\n","        from classification_modules import BARTPK_ctxt as bartmodel\n","        tokenizer = BartTokenizer.from_pretrained(args.bart_model_path)\n","        model_config = BartConfig.from_pretrained(args.bart_model_path)\n","        model = bartmodel(model_config)\n","        model.to(args.device)\n","        if args.bart_model_path == \"facebook/bart-base\" or \"facebook/bart-large\":\n","            add_special_tokens_(model, tokenizer)\n","\n","    else:\n","        raise NotImplementedError\n","\n","    optimizer = AdamW(model.parameters(), lr=args.lr, correct_bias=True)\n","\n","    if args.fp16:\n","        from apex import amp  # Apex is only required if we use fp16 training\n","        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16)\n","    if args.distributed:\n","        model = DistributedDataParallel(model, device_ids=[local_rank], output_device=local_rank)\n","\n","    logger.info(\"Prepare datasets\")\n","    train_loader, val_loader, train_sampler, valid_sampler = get_data_loaders(args, tokenizer)\n","\n","    # Training function and trainer\n","    def update(engine, batch):\n","        model.train()\n","        batch = tuple(input_tensor.to(args.device) for input_tensor in batch)\n","        if model.config.model_type == 'gpt2':\n","            input_ids, input_eos, lm_labels, token_type_ids, mc_token_ids, persona_candidates, persona_can_idx, persona_grounding, knowledge_candidates, \\\n","            knowledge_can_idx, knowledge_grounding, tot_knowledge, tot_knowledge_token_ids, tot_knowledge_eos, reply, dialog, dialog_tti = batch\n","            output = model(\n","                input_ids=input_ids,\n","                input_eos=input_eos,\n","                token_type_ids=token_type_ids,\n","                only_dial_input_ids=dialog,\n","                only_dial_token_type_ids=dialog_tti,\n","                persona_input_ids=persona_candidates,\n","                knowledge_input_ids=knowledge_candidates,\n","                persona_can_idx=persona_can_idx,\n","                persona_grounding=persona_grounding,\n","                knowledge_can_idx=knowledge_can_idx,\n","                knowledge_grounding=knowledge_grounding,\n","                tot_knowledge=tot_knowledge,\n","                tot_knowledge_token_ids=tot_knowledge_token_ids,\n","                tot_knowledge_eos=tot_knowledge_eos,\n","                training=True,\n","                mc_token_ids=mc_token_ids\n","            )\n","        elif model.config.model_type == 'bart':\n","          # dialogue token ids, dialogue labels, persona token ids, persona labels, knowledge token ids, knowledge labels\n","            input_ids, input_eos, decoder_input_ids, lm_labels, token_type_ids, mc_token_ids, persona_candidates, persona_can_idx, persona_grounding, knowledge_candidates, \\\n","            knowledge_can_idx, knowledge_grounding, tot_knowledge, tot_knowledge_eos, reply, dialog, filtered_triples, ner_cans, triple_plus_knowledge = batch\n","            # import pdb; pdb.set_trace()\n","            assert len(filtered_triples) == len(knowledge_candidates)\n","            output = model(\n","                input_ids=input_ids,\n","                input_eos=input_eos,\n","                only_dial_input_ids=dialog,\n","                decoder_input_ids=decoder_input_ids,\n","                persona_input_ids=persona_candidates,\n","                knowledge_input_ids=knowledge_candidates if not args.replace_k_triples else filtered_triples \\\n","                    if not args.append_k_triples else triple_plus_knowledge,\n","                persona_can_idx=persona_can_idx,\n","                persona_grounding=persona_grounding,\n","                knowledge_can_idx=knowledge_can_idx,\n","                knowledge_grounding=knowledge_grounding,\n","                tot_knowledge=tot_knowledge,\n","                tot_knowledge_eos=tot_knowledge_eos,\n","                lm_labels=lm_labels,\n","                training=True,\n","                mc_token_ids=mc_token_ids\n","            )\n","        else:\n","            raise NotImplementedError\n","        lm_loss, knowledge_loss, persona_loss = output[0], output[1], output[2]\n","        loss = (lm_loss * args.lm_coef + knowledge_loss * args.kn_coef + persona_loss * args.ps_coef) / args.gradient_accumulation_steps\n","        if args.fp16:\n","            with amp.scale_loss(loss, optimizer) as scaled_loss:\n","                scaled_loss.backward()\n","            torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_norm)\n","        else:\n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_norm)\n","        if engine.state.iteration % args.gradient_accumulation_steps == 0:\n","            optimizer.step()\n","            optimizer.zero_grad()\n","        return (lm_loss.item(), knowledge_loss.item(), persona_loss.item())\n","\n","    trainer = Engine(update)\n","\n","    # Evaluation function and evaluator (evaluator output is the input of the metrics)\n","    def inference(engine, batch):\n","        model.eval()\n","        with torch.no_grad():\n","            batch = tuple(input_tensor.to(args.device) for input_tensor in batch)\n","            if model.config.model_type == 'gpt2':\n","                input_ids, input_eos, lm_labels, token_type_ids, mc_token_ids, persona_candidates, persona_can_idx, persona_grounding, knowledge_candidates, \\\n","                knowledge_can_idx, knowledge_grounding, tot_knowledge, tot_knowledge_token_ids, tot_knowledge_eos, reply, dialog, dialog_tti = batch\n","\n","                output = model(\n","                    input_ids=input_ids,\n","                    input_eos=input_eos,\n","                    token_type_ids=token_type_ids,\n","                    only_dial_input_ids=dialog,\n","                    only_dial_token_type_ids=dialog_tti,\n","                    persona_input_ids=persona_candidates,\n","                    knowledge_input_ids=knowledge_candidates,\n","                    persona_can_idx=persona_can_idx,\n","                    knowledge_can_idx=knowledge_can_idx,\n","                    tot_knowledge=tot_knowledge,\n","                    tot_knowledge_token_ids=tot_knowledge_token_ids,\n","                    tot_knowledge_eos=tot_knowledge_eos,\n","                    training=False,\n","                    mc_token_ids=mc_token_ids\n","                )\n","                lm_labels, lm_logits, knowledge_logits, persona_logits = output[0], output[1], output[2], output[3]\n","\n","\n","            elif model.config.model_type == 'bart':\n","                input_ids, input_eos, decoder_input_ids, lm_labels, token_type_ids, mc_token_ids, persona_candidates, persona_can_idx, persona_grounding, knowledge_candidates, \\\n","                knowledge_can_idx, knowledge_grounding, tot_knowledge, tot_knowledge_eos, reply, dialog, filtered_triples, ner_cans, triple_plus_knowledge = batch\n","                assert len(filtered_triples) == len(knowledge_candidates)\n","                output = model(\n","                    input_ids=input_ids,\n","                    input_eos=input_eos,\n","                    only_dial_input_ids=dialog,\n","                    decoder_input_ids=decoder_input_ids,\n","                    persona_input_ids=persona_candidates,\n","                    knowledge_input_ids=knowledge_candidates if not args.replace_k_triples else filtered_triples \\\n","                        if not args.append_k_triples else triple_plus_knowledge,\n","                    persona_can_idx=persona_can_idx,\n","                    knowledge_can_idx=knowledge_can_idx,\n","                    tot_knowledge=tot_knowledge,\n","                    tot_knowledge_eos=tot_knowledge_eos,\n","                    training=False,\n","                    mc_token_ids=mc_token_ids\n","                )\n","                lm_logits, knowledge_logits, persona_logits = output[0], output[1], output[2]\n","\n","\n","            else:\n","                raise NotImplementedError\n","\n","            lm_logits_flat_shifted = lm_logits[:, :-1, :].contiguous().view(-1, lm_logits.size(-1))\n","            lm_labels_flat_shifted = lm_labels[:, 1:].contiguous().view(-1)\n","\n","            persona_logits = persona_logits.squeeze()\n","            persona_grounding = persona_grounding.type_as(persona_logits).squeeze()\n","\n","            sigmoid = Sigmoid()\n","            persona_pred_sigmoid = sigmoid(persona_logits)\n","            persona_pred_sigmoid = (persona_pred_sigmoid > 0.5).float()\n","\n","            softmax = Softmax(dim=-1)\n","            knowledge_pred = softmax(knowledge_logits)\n","            _, k_index_1 = torch.topk(knowledge_pred, k=1, dim=-1)\n","            _, k_index_5 = torch.topk(knowledge_pred, k=5, dim=-1)\n","            k_index_1, k_index_5 = k_index_1.squeeze(0), k_index_5.squeeze(0)\n","            k_index_1_cvtd = torch.tensor([1 if num in k_index_1 else 0 for num in range(10)], device=args.device)\n","            k_label_cvtd = torch.tensor([1 if num in knowledge_grounding else 0 for num in range(10)],\n","                                        device=args.device)\n","\n","            lm_pred = softmax(lm_logits_flat_shifted)\n","            lm_val, lm_idx = torch.topk(lm_pred, k=1, dim=-1)\n","            lm_idx = lm_idx.squeeze(-1)\n","\n","            mask = (lm_labels_flat_shifted != -100)\n","            lm_labels_only = [lm_labels_flat_shifted[mask].tolist()]\n","            lm_idx_only = lm_idx[mask].tolist()\n","\n","            return (lm_logits_flat_shifted, knowledge_logits, persona_logits, persona_pred_sigmoid, k_index_1_cvtd, knowledge_pred, lm_idx_only), \\\n","                   (lm_labels_flat_shifted, knowledge_grounding, persona_grounding.type_as(persona_logits), k_label_cvtd, lm_labels_only)\n","\n","\n","    evaluator = Engine(inference)\n","\n","    # Attach evaluation to trainer: we evaluate when we start the training and at the end of each epoch\n","    trainer.add_event_handler(Events.EPOCH_COMPLETED, lambda _: evaluator.run(val_loader))\n","    if args.n_epochs < 1:\n","        trainer.add_event_handler(Events.COMPLETED, lambda _: evaluator.run(val_loader))\n","    if args.eval_before_start:\n","        trainer.add_event_handler(Events.STARTED, lambda _: evaluator.run(val_loader))\n","\n","    # Make sure distributed data samplers split the dataset nicely between the distributed processes\n","    if args.distributed:\n","        trainer.add_event_handler(Events.EPOCH_STARTED, lambda engine: train_sampler.set_epoch(engine.state.epoch))\n","        evaluator.add_event_handler(Events.EPOCH_STARTED, lambda engine: valid_sampler.set_epoch(engine.state.epoch))\n","\n","    # Linearly decrease the learning rate from lr to zero\n","    scheduler = PiecewiseLinear(optimizer, \"lr\", [(0, args.lr), (args.n_epochs * len(train_loader), 0.0)])\n","    trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n","\n","    # Prepare metrics - note how we compute distributed metrics\n","    RunningAverage(output_transform=lambda x: x[0]).attach(trainer, \"lm_loss\")\n","    RunningAverage(output_transform=lambda x: x[1]).attach(trainer, \"knowledge_loss\")\n","    RunningAverage(output_transform=lambda x: x[2]).attach(trainer, \"persona_loss\")\n","\n","    metrics = {\n","        \"lm_loss\": Loss(torch.nn.CrossEntropyLoss(ignore_index=-100), output_transform=lambda x: (x[0][0], x[1][0])),\n","        \"knowledge_loss\": Loss(torch.nn.CrossEntropyLoss(), output_transform=lambda x: (x[0][1], x[1][1])),\n","        \"persona_loss\": Loss(torch.nn.BCEWithLogitsLoss(), output_transform=lambda x: (x[0][2], x[1][2])),\n","        \"Knowledge_acc\": Accuracy(output_transform=lambda x: (x[0][5], x[1][3])),\n","        \"Persona_acc\":Accuracy(output_transform=lambda x: (x[0][3], x[1][2]))}\n","\n","    metrics.update({\"average_lm_loss\": MetricsLambda(average_distributed_scalar, metrics[\"lm_loss\"], args),\n","                    \"average_knowledge_loss\": MetricsLambda(average_distributed_scalar, metrics[\"knowledge_loss\"],args),\n","                    \"average_persona_loss\": MetricsLambda(average_distributed_scalar, metrics[\"persona_loss\"], args),\n","                    \"average_Knowledge_acc\": MetricsLambda(average_distributed_scalar, metrics[\"Knowledge_acc\"], args),\n","                    \"average_Persona_acc\": MetricsLambda(average_distributed_scalar, metrics[\"Persona_acc\"], args)\n","                    })\n","\n","    metrics[\"average_ppl\"] = MetricsLambda(math.exp, metrics[\"average_lm_loss\"])\n","\n","    for name, metric in metrics.items():\n","        metric.attach(evaluator, name)\n","\n","    # On the main process: add progress bar, tensorboard, checkpoints and save model, configuration and tokenizer before we start to train\n","    if args.local_rank in [-1, 0]:\n","        pbar = ProgressBar(persist=True)\n","        pbar.attach(trainer, metric_names=[\"lm_loss\"])\n","        evaluator.add_event_handler(Events.COMPLETED,\n","                                    lambda _: pbar.log_message(\"Validation: %s\" % pformat(evaluator.state.metrics)))\n","\n","        dir_name = str(os.path.basename(__file__))[:-3] + \"_\" + args.model_name + \"_\" + args.flag\n","        log_dir = make_focus_logdir(dir_name)\n","        tb_logger = TensorboardLogger(log_dir)\n","\n","        tb_logger.attach(trainer, log_handler=OutputHandler(tag=\"training\", metric_names=[\"lm_loss\", \"knowledge_loss\", \"persona_loss\",\n","                                                                          \"knowledge_accuracy\", \"persona_accuracy\", \"f1_score\"]),\n","                         event_name=Events.ITERATION_COMPLETED)\n","        tb_logger.attach(trainer, log_handler=OptimizerParamsHandler(optimizer), event_name=Events.ITERATION_STARTED)\n","        tb_logger.attach(evaluator, log_handler=OutputHandler(tag=\"validation\", metric_names=list(metrics.keys())),\n","                         event_name=Events.ITERATION_STARTED)\n","        tb_logger.attach(evaluator, log_handler=OutputHandler(tag=\"validation\", metric_names=list(metrics.keys())),\n","                         event_name=Events.EPOCH_STARTED)\n","        tb_logger.attach(evaluator, log_handler=OutputHandler(tag=\"validation\", metric_names=list(metrics.keys())),\n","                         event_name=Events.ITERATION_COMPLETED(every=5000))\n","        checkpoint_handler = ModelCheckpoint(log_dir, 'checkpoint', n_saved=3)\n","        trainer.add_event_handler(Events.EPOCH_COMPLETED, checkpoint_handler, {\n","            'mymodel': getattr(model, 'module', model)})  # \"getattr\" takes care of distributed encapsulation\n","\n","        torch.save(args, log_dir + '/model_training_args.bin')\n","        getattr(model, 'module', model).config.to_json_file(os.path.join(log_dir, CONFIG_NAME))\n","        tokenizer.save_pretrained(log_dir)\n","\n","    # Run the training\n","    trainer.run(train_loader, max_epochs=args.n_epochs)\n","\n","    # On the main process: close tensorboard logger and rename the last checkpoint (for easy re-loading with OpenAIGPTModel.from_pretrained method)\n","    if args.local_rank in [-1, 0] and args.n_epochs > 0:\n","        os.rename(os.path.join(log_dir, checkpoint_handler._saved[-1][1]), os.path.join(log_dir, WEIGHTS_NAME))\n","        tb_logger.close()\n","\n","\n","if __name__ == \"__main__\":\n","    train()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wlNVn_e5xQmR"},"outputs":[],"source":["#(c) 2021 NCSOFT Corporation & Korea University. All rights reserved.\n","import logging\n","import random\n","from argparse import ArgumentParser\n","from pprint import pformat\n","from tqdm import tqdm\n","import torch\n","import torch.nn.functional as F\n","from torch.nn import Sigmoid, Softmax, CrossEntropyLoss\n","from data_utils import get_testdata_loaders, add_special_tokens_\n","from inference_test import sample_sequence\n","from ignite.metrics import Accuracy\n","from datasets import load_metric\n","from torchmetrics import CHRFScore\n","from rouge_score import rouge_scorer\n","logger = logging.getLogger(__file__)\n","from setproctitle import setproctitle\n","setproctitle('Yoonna')\n","\n","SPECIAL_TOKENS = [\"<machine>\", \"<human>\", \"<persona>\", \"<knowledge>\"]\n","\n","# TODO : Augment with https://en.wikipedia.org/wiki/Most_common_words_in_English\n","# Rough version allowing knowledge duplicates\n","\n","def run():\n","    parser = ArgumentParser()\n","    parser.add_argument(\"--test_dataset_path\", type=str, default=\"data/new_valid_json_v3.json\", help=\"Path or url of the dataset. If empty download from S3.\") #data\n","    parser.add_argument(\"--test_dataset_cache\", type=str, default='data/new_focus_cache_v5_ner_wo_rel.tar.gz', help=\"Path or url of the dataset cache\")\n","    parser.add_argument(\"--model_name\", type=str, default=\"\", help=\"{GPT2, BART, transformer-decoder, transformer-encdec}\")\n","    parser.add_argument(\"--model_checkpoint\", type=str, default=\"\", help=\"Path, url or short name of the model\")\n","    parser.add_argument(\"--max_history\", type=int, default=1, help=\"Number of previous utterances to keep in history\")\n","    parser.add_argument(\"--test_batch_size\", type=int, default=1, help=\"Batch size for testing\")\n","    parser.add_argument(\"--device\", type=str, default=\"cuda\" if torch.cuda.is_available() else \"cpu\", help=\"Device (cuda or cpu)\")\n","    parser.add_argument(\"--no_sample\", action='store_true', help=\"Set to use greedy decoding instead of sampling\")\n","    parser.add_argument(\"--max_length\", type=int, default=20, help=\"Maximum length of the output utterances\")\n","    parser.add_argument(\"--min_length\", type=int, default=1, help=\"Minimum length of the output utterances\")\n","    parser.add_argument(\"--select_max_len\", action='store_true', help=\"Select maximum length beam\")\n","    parser.add_argument(\"--inference\", action='store_true', help=\"If true, inference with gold knowledge\")\n","    parser.add_argument(\"--seed\", type=int, default=19950604, help=\"Seed\")\n","    parser.add_argument(\"--temperature\", type=int, default=0.7, help=\"Sampling softmax temperature\")\n","    parser.add_argument(\"--top_k\", type=int, default=0, help=\"Filter top-k tokens before sampling (<=0: no filtering)\")\n","    parser.add_argument(\"--top_p\", type=float, default=0.9, help=\"Nucleus filtering (top-p) before sampling (<=0.0: no filtering)\")\n","    parser.add_argument(\"--local_rank\", type=int, default=-1, help=\"Local rank for distributed training (-1: not distributed)\")\n","    parser.add_argument(\"--literal\", type=float, default=1.0, help=\"Constraining based on literal\")\n","    parser.add_argument(\"--graph\", type=float, default=1.0, help=\"Constraining based on graph\")\n","    parser.add_argument(\"--ner\", type=float, default=1.0, help=\"Constraining based on NER\")\n","    parser.add_argument(\"--eos_std_adj\", type=float, default=0.0, help=\"Equation to adjust EOS token probability\")\n","    parser.add_argument(\"--markov_graph\", type=float, default=1.0, help=\"Constraining based on markov characteristics of graph\")\n","    parser.add_argument(\"--markov_graph_v2\", type=float, default=1.0, help=\"Constraining based on graph subject utilizing all relation / objects\")\n","    parser.add_argument(\"--constrain\", action='store_true', help=\"New constraining mechanism\")\n","    parser.add_argument(\"--gen_eval\", action='store_true', help=\"Generation Auto Evaluation\")\n","    parser.add_argument(\"--without_rel\", action=\"store_true\", help=\"Whether to remove relation from graph data\")\n","    parser.add_argument(\"--submit_path\", type=str, default=\"\", help=\"Line-by-line output for submission\")\n","    parser.add_argument(\"--beam_size\", type=int, default=1, help=\"Beam size (currently only performs basic override)\")\n","    parser.add_argument(\"--beam_length_alpha\", type=float, default=0.0, help=\"Beam alpha discount\")\n","    parser.add_argument(\"--no_eval\", action='store_true', help=\"No evaluation mode.\")\n","    parser.add_argument(\"--line_proc\", action='store_true', help=\"If true, the file is processed into lines.\")\n","    args = parser.parse_args()\n","\n","    logging.basicConfig(level=logging.INFO)\n","    logger = logging.getLogger(__file__)\n","    logger.info(pformat(args))\n","    args.distributed = (args.local_rank != -1)\n","\n","    if args.seed != 0:\n","    \trandom.seed(args.seed)\n","        torch.random.manual_seed(args.seed)\n","        torch.cuda.manual_seed(args.seed)\n","\n","    logger.info(\"Get model and tokenizer\")\n","    logger.info(f\"Submit to : {args.submit_path}\")\n","\n","    if args.model_name == 'GPT2':\n","        from transformers import GPT2Tokenizer\n","        from classification_modules import GPT2PK_ctxt\n","        tokenizer = GPT2Tokenizer.from_pretrained(args.model_checkpoint)\n","        model = GPT2PK_ctxt.from_pretrained(args.model_checkpoint)\n","        model.to(args.device)\n","        add_special_tokens_(model, tokenizer)\n","\n","    elif args.model_name == 'BART':\n","        from transformers import BartTokenizer\n","        from classification_modules import BARTPK_ctxt\n","        tokenizer = BartTokenizer.from_pretrained(args.model_checkpoint)\n","        model = BARTPK_ctxt.from_pretrained(args.model_checkpoint)\n","        logger.info(\"Model loaded\")\n","\n","        model.to(args.device)\n","        logger.info(\"To Device\")\n","        add_special_tokens_(model, tokenizer)\n","\n","    elif args.model_name == 'transformer-decoder':\n","        from transformers import GPT2Tokenizer\n","        from classification_modules import GPT2PK_ctxt\n","        tokenizer = GPT2Tokenizer.from_pretrained(args.model_checkpoint)\n","        model = GPT2PK_ctxt.from_pretrained(args.model_checkpoint)\n","        model.to(args.device)\n","        add_special_tokens_(model, tokenizer)\n","\n","    elif args.model_name == 'transformer-encdec':\n","        from transformers import BartTokenizer\n","        from classification_modules import BARTPK_ctxt\n","        tokenizer = BartTokenizer.from_pretrained(args.model_checkpoint)\n","        model = BARTPK_ctxt.from_pretrained(args.model_checkpoint)\n","        model.to(args.device)\n","        add_special_tokens_(model, tokenizer)\n","\n","    else:\n","        raise NotImplementedError\n","\n","\n","    #dataset = get_dataset_only_train(tokenizer, args.dataset_path, args.dataset_cache)\n","    logger.info(\"Prepare datasets\")\n","    test_loader, test_sampler = get_testdata_loaders(args, tokenizer, generation=True)\n","\n","    with torch.no_grad():\n","        r1 = 0\n","        r2 = 0\n","        rl = 0\n","        bleu = 0\n","        bertscore_precision = 0\n","        bertscore_recall = 0\n","        bertscore_f1 = 0\n","        # bleurt = 0\n","        mauve = 0\n","        chrf1 = 0\n","        rouge_metric = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n","        bleu_metric = load_metric(\"sacrebleu\")\n","        bertscore_metric = load_metric(\"bertscore\")\n","        # bleurt_metric = load_metric(\"bleurt\")\n","        # mauve_metric = load_metric(\"mauve\")\n","        chrf_metric = CHRFScore()\n","\n","        pg = Accuracy()\n","        kg = Accuracy()\n","\n","        pass_num = 0\n","        test_pass_list = []\n","\n","        import collections\n","        submit_output = collections.defaultdict(list)\n","\n","        # import pdb; pdb.set_trace()\n","        for test_data_index, test_data in enumerate(tqdm(test_loader)):\n","            # import pdb; pdb.set_trace()\n","            loss_fct = CrossEntropyLoss(ignore_index=-100)\n","            # TODO : Update can_idx for knowledge and persona when testing\n","            if model.config.model_type == 'gpt2':\n","                input_ids, input_eos, lm_labels, token_type_ids, mc_token_ids, persona_candidates, persona_can_idx, persona_grounding, knowledge_candidates, \\\n","                knowledge_can_idx, knowledge_grounding, tot_knowledge, tot_knowledge_token_ids, tot_knowledge_eos, reply, dialog, dialog_tti = test_data\n","\n","            elif model.config.model_type == 'bart':\n","                # import pdb; pdb.set_trace()\n","                input_ids, input_eos, decoder_input_ids, lm_labels, token_type_ids, mc_token_ids, persona_candidates, persona_can_idx, persona_grounding, knowledge_candidates, \\\n","                knowledge_can_idx, knowledge_grounding, tot_knowledge, tot_knowledge_eos, reply, dialog, dialog_ID, filtered_triples, filtered_triples_raw,\\\n","                ner_cans = test_data\n","                # import pdb; pdb.set_trace()\n","            else:\n","                raise NotImplementedError\n","\n","            reply = reply[0]\n","            mask = (reply != tokenizer.pad_token_id)\n","            reply = reply[mask]\n","            # import pdb; pdb.set_trace()\n","\n","            #print('reply.tolist() ', reply.tolist())\n","            if len(reply.tolist()) == 0:\n","                pass_num += 1\n","                test_pass_list.append(test_data_index)\n","                continue\n","\n","            if model.config.model_type == 'gpt2':\n","                output = model(\n","                    input_ids=input_ids,\n","                    input_eos=input_eos,\n","                    token_type_ids=token_type_ids,\n","                    only_dial_input_ids=dialog,\n","                    only_dial_token_type_ids=dialog_tti,\n","                    persona_input_ids=persona_candidates,\n","                    knowledge_input_ids=knowledge_candidates,\n","                    persona_can_idx=persona_can_idx,\n","                    knowledge_can_idx=knowledge_can_idx,\n","                    tot_knowledge=tot_knowledge,\n","                    tot_knowledge_token_ids=tot_knowledge_token_ids,\n","                    tot_knowledge_eos=tot_knowledge_eos,\n","                    training=False,\n","                    mc_token_ids=mc_token_ids\n","                )\n","                lm_labels, lm_logits, knowledge_logits, persona_logits = output[0], output[1], output[2], output[3]\n","\n","                machine, human, persona, knowledge, padding, bos = 50257, 50258, 50259, 50260, 50261, 50256\n","                device = input_ids.get_device()\n","\n","                machine_tensor = torch.tensor([machine]).cuda(device)\n","                persona_tensor = torch.tensor([persona]).cuda(device)\n","                knowledge_tensor = torch.tensor([knowledge]).cuda(device)\n","                bos_tensor = torch.tensor([bos]).cuda(device)\n","\n","\n","                sigmoid = Sigmoid()\n","                persona_pred_sigmoid = sigmoid(persona_logits)\n","                persona_pred_sigmoid = (persona_pred_sigmoid > 0.5).float()\n","                all_persona_pred = []\n","                selected_persona_idx = list()\n","                for batch_idx, persona_batch in enumerate(torch.eq(persona_pred_sigmoid, 1)):\n","                    batch_list_idx = list()\n","                    batch_list = list()\n","                    for i, can in enumerate(persona_batch):\n","                        if can == True:\n","                            batch_list_idx.append(can)\n","                            persona_selected_now = persona_candidates[batch_idx][i]\n","                            mask_persona = torch.ne(persona_selected_now, padding)\n","                            persona_selected_now = torch.masked_select(persona_selected_now, mask_persona)\n","                            batch_list.append(persona_selected_now[:-2])\n","                    all_persona_pred.append(batch_list)\n","                    selected_persona_idx.append(batch_list_idx)\n","                p_index_cvtd = persona_pred_sigmoid\n","\n","\n","                softmax = Softmax(dim=-1)\n","                knowledge_softmax = softmax(knowledge_logits)\n","                _, k_index_1 = torch.topk(knowledge_softmax, k=1, dim=-1)\n","                all_knowledge_pred = []\n","                for batch_i in range(args.test_batch_size):\n","                    knowledge_pred_idx = k_index_1[batch_i]\n","                    knowledge_pred = knowledge_candidates[batch_i][knowledge_pred_idx]\n","                    mask_knowledge = torch.ne(knowledge_pred, padding)\n","                    knowledge_pred = torch.masked_select(knowledge_pred, mask_knowledge)\n","                    knowledge_pred = knowledge_pred[1:-2]\n","                    all_knowledge_pred.append(knowledge_pred) #delete bos, knowledge_st, eos\n","\n","\n","                k_index_1 = k_index_1.squeeze(0)\n","                k_index_cvtd = torch.tensor([1 if num in k_index_1 else 0 for num in range(10)], device=args.device)\n","\n","                final_input_list = []\n","                final_input_tti_list = []\n","                for batch_i in range(args.test_batch_size):\n","                    only_dial_input_ids_batch = dialog[batch_i]\n","                    only_dial_token_type_ids_batch = dialog_tti[batch_i]\n","                    mask_only_dial_input_ids_batch = torch.ne(only_dial_input_ids_batch, padding)\n","                    mask_only_dial_tti_batch = torch.ne(only_dial_token_type_ids_batch, padding)\n","                    only_dial_input_ids_batch = torch.masked_select(only_dial_input_ids_batch, mask_only_dial_input_ids_batch)\n","                    only_dial_token_type_ids_batch = torch.masked_select(only_dial_token_type_ids_batch, mask_only_dial_tti_batch)\n","\n","                    if len(all_persona_pred[batch_i]) > 0:\n","                        concat_persona = torch.cat(all_persona_pred[batch_i], dim=-1)\n","                        new_persona = torch.cat([persona_tensor, concat_persona], dim=-1)\n","                        new_persona_tti = torch.tensor([persona] * (new_persona.size()[0])).cuda(device)\n","\n","                    else:\n","                        new_persona = None\n","                        new_persona_tti = None\n","\n","\n","                    new_knowledge = torch.cat([knowledge_tensor, all_knowledge_pred[batch_i]], dim=-1)\n","                    new_knowledge_tti = torch.tensor([knowledge] * (new_knowledge.size()[0])).cuda(device)\n","\n","                    only_dial_input_ids_batch = only_dial_input_ids_batch[1:-1]\n","                    only_dial_token_type_ids_batch = only_dial_token_type_ids_batch[1:]\n","                    if new_persona is not None:\n","                        new_input = torch.cat([bos_tensor, new_knowledge, new_persona, only_dial_input_ids_batch, machine_tensor], dim=-1)\n","                        new_input_tti = torch.cat([knowledge_tensor, new_knowledge_tti, new_persona_tti, only_dial_token_type_ids_batch], dim=-1)\n","                    else:\n","                        new_input = torch.cat([bos_tensor, new_knowledge, only_dial_input_ids_batch, machine_tensor], dim=-1)\n","                        new_input_tti = torch.cat([knowledge_tensor, new_knowledge_tti, only_dial_token_type_ids_batch], dim=-1)\n","\n","                    final_input_list.append(new_input)\n","                    final_input_tti_list.append(new_input_tti)\n","                final_input_tensor = torch.stack(final_input_list)\n","                final_input_tti_tensor = torch.stack(final_input_tti_list)\n","\n","                out_ids = sample_sequence(final_input_tensor, token_type_ids=final_input_tti_tensor,\n","                                          decoder_input_ids=None, tokenizer=tokenizer, model=model, args=args, current_output=None)\n","\n","            elif model.config.model_type == 'bart':\n","                # import pdb; pdb.set_trace()\n","                output = model(\n","                    input_ids=input_ids,\n","                    input_eos=input_eos,\n","                    only_dial_input_ids=dialog,\n","                    decoder_input_ids=decoder_input_ids,\n","                    persona_input_ids=persona_candidates,\n","                    knowledge_input_ids=knowledge_candidates,\n","                    persona_can_idx=persona_can_idx,\n","                    knowledge_can_idx=knowledge_can_idx,\n","                    tot_knowledge=tot_knowledge,\n","                    tot_knowledge_eos=tot_knowledge_eos,\n","                    training=False,\n","                    mc_token_ids=mc_token_ids\n","                )\n","                # import pdb; pdb.set_trace()\n","                lm_logits, knowledge_logits, persona_logits = output[0], output[1], output[2]\n","\n","                persona, knowledge = 50267, 50268\n","                bos, padding, eos = 0, 1, 2\n","                device = input_ids.get_device()\n","\n","                # import pdb; pdb.set_trace()\n","                persona_tensor = torch.tensor([persona]).cuda(device)\n","                knowledge_tensor = torch.tensor([knowledge]).cuda(device)\n","                bos_tensor = torch.tensor([bos]).cuda(device)\n","                eos_tensor = torch.tensor([eos]).cuda(device)\n","                max_position = 1024\n","\n","                # import pdb; pdb.set_trace()\n","                sigmoid = Sigmoid()\n","                persona_pred_sigmoid = sigmoid(persona_logits)\n","                persona_pred_sigmoid = (persona_pred_sigmoid > 0.5).float()\n","                all_persona_pred = []\n","                selected_persona_idx = list()\n","                for batch_idx, persona_batch in enumerate(torch.eq(persona_pred_sigmoid, 1)):\n","                    batch_list_idx = list()\n","                    batch_list = list()\n","                    for i, can in enumerate(persona_batch):\n","                        if can == True:\n","                            batch_list_idx.append(can)\n","                            persona_selected_now = persona_candidates[batch_idx][i]\n","                            mask_persona = torch.ne(persona_selected_now, padding)\n","                            persona_selected_now = torch.masked_select(persona_selected_now, mask_persona)\n","                            batch_list.append(persona_selected_now[:-2])\n","                    all_persona_pred.append(batch_list)\n","                    selected_persona_idx.append(batch_list_idx)\n","\n","                p_index_cvtd = persona_pred_sigmoid\n","\n","                # import pdb; pdb.set_trace()\n","                softmax = Softmax(dim=-1)\n","                knowledge_softmax = softmax(knowledge_logits)\n","                _, k_index_1 = torch.topk(knowledge_softmax, k=1, dim=-1)\n","                all_knowledge_pred = []\n","                # all_k_graph_pred = []\n","                # all_ner_pred = []\n","                # all_k_graph_raw_pred = []\n","                # import pdb; pdb.set_trace()\n","                for batch_i in range(args.test_batch_size):\n","                    knowledge_pred_idx = k_index_1[batch_i]\n","                    # import pdb; pdb.set_trace()\n","                    knowledge_pred = knowledge_candidates[batch_i][knowledge_pred_idx]\n","                    # import pdb; pdb.set_trace()\n","                    mask_knowledge = torch.ne(knowledge_pred, padding)\n","                    knowledge_pred = torch.masked_select(knowledge_pred, mask_knowledge)\n","                    knowledge_pred = knowledge_pred[1:-2]\n","                    all_knowledge_pred.append(knowledge_pred) #delete bos, knowledge_st, eos\n","\n","                    # import pdb; pdb.set_trace()\n","                    # k_graph_pred = filtered_triples[batch_i][knowledge_pred_idx]\n","                    # mask_k_graph = torch.ne(k_graph_pred, padding)\n","                    # k_graph_pred = torch.masked_select(k_graph_pred, mask_k_graph)\n","                    # all_k_graph_pred.append(k_graph_pred)\n","                    #\n","                    # import pdb; pdb.set_trace()\n","                    # ner_pred = ner_cans[batch_i][knowledge_pred_idx]\n","                    # mask_ner = torch.ne(ner_pred, padding)\n","                    # ner_pred = torch.masked_select(ner_pred, mask_ner)\n","                    # all_ner_pred.append(ner_pred)\n","\n","                    # TODO : Any processing?\n","                    # all_k_graph_raw_pred.append([[tokenizer.decode(value) for value in triple] for triple in filtered_triples_raw[batch_i]])\n","\n","                # import pdb; pdb.set_trace()\n","                # import pdb; pdb.set_trace()\n","\n","                final_input_list = []\n","                for batch_i in range(args.test_batch_size):\n","                    only_dial_input_ids_batch = dialog[batch_i]\n","                    mask_only_dial_input_ids_batch = torch.ne(only_dial_input_ids_batch, padding)\n","                    only_dial_input_ids_batch = torch.masked_select(only_dial_input_ids_batch, mask_only_dial_input_ids_batch)\n","                    if len(all_persona_pred[batch_i])>0:\n","                        concat_persona = torch.cat(all_persona_pred[batch_i], dim=-1)\n","                        new_persona = torch.cat([persona_tensor, concat_persona], dim=-1)\n","                    else:\n","                        new_persona = None\n","                    new_knowledge = torch.cat([knowledge_tensor, all_knowledge_pred[batch_i]], dim=-1)\n","\n","                    if new_persona is not None:\n","                        new_input = torch.cat([bos_tensor, new_knowledge, new_persona, only_dial_input_ids_batch, eos_tensor], dim=-1)\n","                    else:\n","                        new_input = torch.cat([bos_tensor, new_knowledge, only_dial_input_ids_batch, eos_tensor], dim=-1)\n","                    new_input_size = new_input.size()[0]\n","\n","                    if new_input_size < int(max_position) :\n","                        padding_size = int(max_position) -new_input_size\n","                        add_padding = torch.tensor([padding]*padding_size).cuda(device)\n","                        final_input = torch.cat([new_input, add_padding], dim=-1)\n","                    final_input_list.append(final_input)\n","                final_input_tensor = torch.stack(final_input_list)\n","                decoder_input_ids = bos_tensor.unsqueeze(0)\n","                # print(\"Knowledge used : \" + tokenizer.decode(all_knowledge_pred[0]))\n","                # TODO :\n","                # import pdb; pdb.set_trace()\n","                k_graph_ids = [tokenizer.decode(batch_triples, skip_special_tokens=True) for batch_triples in filtered_triples_raw[0]]\n","                k_graph_ids = [[[toks for toks in triple_rep.split(\" [sep] \")] \\\n","                                 for triple_rep in triple_reps.split(\" [triple_sep] \")] for triple_reps in k_graph_ids]\n","                # import pdb; pdb.set_trace()\n","                out_ids = sample_sequence(final_input_tensor, token_type_ids=None, decoder_input_ids=decoder_input_ids,\n","                                          tokenizer=tokenizer, model=model, args=args, current_output=None, ner_cand_ids=[[]],\n","                                          knowledge_cand_ids=all_knowledge_pred, k_graph_cand_ids=[[]],\n","                                          k_graph_multiple_markov_triple_ids=k_graph_ids)\n","            else:\n","                raise ValueError(f\"wrong model : {model.config.model_type}\")\n","\n","            machine, human, persona, knowledge = tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS)\n","            special_tokens_list = [machine, human, persona, knowledge, tokenizer.pad_token_id, tokenizer.bos_token_id, tokenizer.eos_token_id]\n","\n","            # import pdb;pdb.set_trace()\n","            for special_token in special_tokens_list:\n","                print_dialog = [value for value in dialog[0] if value != special_token]\n","                print_ID = [value for value in dialog_ID[0] if value != special_token]\n","                print_out = [value for value in out_ids if value != special_token]\n","            print_ID = tokenizer.decode(print_ID, skip_special_tokens=True)\n","            pred_reply = tokenizer.decode(print_out, skip_special_tokens=True)\n","\n","            # import pdb; pdb.set_trace()\n","            # This is actually performed against BART predictions. Why not keep it?\n","            # PG\n","            p_label_cvtd = torch.tensor([1 if num in persona_grounding else 0 for num in range(5)], device=args.device)\n","            pg.update((p_index_cvtd.squeeze(), p_label_cvtd))\n","            pg_res = pg.compute()\n","\n","            # KG\n","            k_label_cvtd = torch.tensor([1 if num in knowledge_grounding else 0 for num in range(10)], device=args.device)\n","            kg.update((knowledge_softmax, k_label_cvtd))\n","            kg_res = kg.compute()\n","\n","            if args.submit_path:\n","                # TODO : Check speed & memory?\n","                submit_output[print_ID].append({\n","                    \"generation\": pred_reply\n","                })\n","                # import pdb; pdb.set_trace()\n","\n","            # import pdb; pdb.set_trace()\n","            print(f'{print_ID}, question: ', tokenizer.decode(print_dialog, skip_special_tokens=True))\n","            if args.no_eval:\n","                print('pred: ', pred_reply)\n","                print('---------------------------------------')\n","            else:\n","                gold_reply = reply\n","                #print('reply', reply)\n","\n","                gold_reply = tokenizer.decode(gold_reply.tolist(), skip_special_tokens=True)\n","                print('gold: ', gold_reply)\n","                print('pred: ', pred_reply)\n","                print('---------------------------------------')\n","\n","                #ROUGE\n","                r = rouge_metric.score(pred_reply, gold_reply)\n","                r1 += r['rouge1'].fmeasure\n","                r2 += r['rouge2'].fmeasure\n","                rl += r['rougeL'].fmeasure\n","\n","                #BLEU1,2,3,4 / BLEU avg\n","                bleu += bleu_metric.compute(predictions=[pred_reply], references=[[gold_reply]])['score']\n","\n","                if args.gen_eval:\n","                    # print(pred_reply)\n","                    # print(gold_reply)\n","\n","                    #BERTScore\n","                    bertscore_dict = bertscore_metric.compute(predictions=[pred_reply], references=[gold_reply], lang=\"en\")\n","                    bertscore_f1 += sum(bertscore_dict['f1'])\n","                    bertscore_precision += sum(bertscore_dict['precision'])\n","                    bertscore_recall += sum(bertscore_dict['recall'])\n","\n","                    #BLEURT\n","                    # bleurt += sum(bleurt_metric.compute(predictions=[pred_reply], references=[gold_reply])[\"scores\"])\n","\n","                    #MAUVE\n","                    # mauve += mauve_metric.compute(predictions=[pred_reply], references=[gold_reply]).mauve\n","\n","                #CharF1\n","                chrf1 += chrf_metric([pred_reply], [[gold_reply]])\n","\n","        # Result printing out of loop\n","        print('test pass list: ', test_pass_list, 'len: ', pass_num)\n","\n","        if not args.no_eval:\n","            chrf1_result = chrf1/(test_data_index+1-pass_num)\n","            rouge1_result = r1/(test_data_index+1-pass_num)\n","            rouge2_result = r2/(test_data_index+1-pass_num)\n","            rougel_result = rl/(test_data_index+1-pass_num)\n","            bleu_result = bleu/(test_data_index+1-pass_num)\n","            bertscore_f1_result = bertscore_f1/(test_data_index+1-pass_num)\n","            bertscore_precision_result = bertscore_precision/(test_data_index+1-pass_num)\n","            bertscore_recall_result = bertscore_recall/(test_data_index+1-pass_num)\n","            # bleurt_result = bleurt/(test_data_index+1-pass_num)\n","            # mauve_result = mauve/(test_data_index+1-pass_num)\n","            print(\"F1: \", chrf1_result)\n","            print(\"ROUGE1\", rouge1_result)\n","            print(\"ROUGE2\", rouge2_result)\n","            print(\"ROUGEL\", rougel_result)\n","            print(\"avg BLEU: \", bleu_result)\n","            if args.gen_eval:\n","                print(\"BERTScore F1: \", bertscore_f1_result)\n","                print(\"BERTScore Precision: \", bertscore_precision_result)\n","                print(\"BERTScore Recall: \", bertscore_recall_result)\n","                # print(\"BLEURT\", bleurt_result)\n","                # print(\"MAUVE\", mauve_result)\n","\n","        pg_result = pg_res\n","        kg_result = kg_res\n","\n","        print(\"PG: \", pg_result)\n","        print(\"KG: \", kg_result)\n","\n","        submit_path = args.submit_path\n","        if submit_path:\n","            final_out = [{id: gen} for id, gen in submit_output.items()]\n","            import json\n","            with open(submit_path, 'a') as file:\n","                file.write(json.dumps(final_out))\n","\n","\n","if __name__ == \"__main__\":\n","    run()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nenuu5vKxRJI"},"outputs":[],"source":["# inference -> BART -> generation -> ids -> decoding -> text"]},{"cell_type":"code","execution_count":61,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":550,"status":"ok","timestamp":1667290398644,"user":{"displayName":"김민상","userId":"05312793560890503441"},"user_tz":-540},"id":"A87LDPA-qcEv","outputId":"a39ac2ca-0532-4650-a0e7-590d36a449d4"},"outputs":[{"data":{"text/plain":["['[unused0]', '[unused2]', '[unused4]', '[unused4]', '[unused6]']"]},"execution_count":61,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer.convert_ids_to_tokens([1,3,5,5,7])"]},{"cell_type":"code","execution_count":61,"metadata":{"executionInfo":{"elapsed":434,"status":"ok","timestamp":1667290438483,"user":{"displayName":"김민상","userId":"05312793560890503441"},"user_tz":-540},"id":"aRuqOPEzQHv0"},"outputs":[],"source":["# data -> persona, dialogue, knowledge -> special tokens -> LM train -> decoding (inference)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyN0uLc4CZJqqI8JF8d9jn2H","collapsed_sections":[],"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
